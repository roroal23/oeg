{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Fine-Tuning BERT for Sentiment Analysis",
   "id": "d733fd402d68f34a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Siguiendo los pasos que aparecen en \"A Complete Guide to BERT with Code\"",
   "id": "324229f400d5006e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T12:24:22.680375Z",
     "start_time": "2026-02-09T12:24:22.572153Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ],
   "id": "cb8be2ba4135b5b0",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-09T12:24:22.978028Z",
     "start_time": "2026-02-09T12:24:22.690726Z"
    }
   },
   "source": "import pandas as pd",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T12:24:23.369982Z",
     "start_time": "2026-02-09T12:24:22.991268Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('data/IMDB Dataset.csv')\n",
    "df.head(5)"
   ],
   "id": "c008800aedc23bf3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1 Load and Preprocess the Dataset",
   "id": "7ca3077ff828648a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cleaning the reviews",
   "id": "a36bab22f5f482a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T12:24:25.135061Z",
     "start_time": "2026-02-09T12:24:23.388093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Original:\\n{df.iloc[1]['review'][0:72]}\")\n",
    "\n",
    "#Remove HTML break tags <br />\n",
    "df['review_cleaned'] = df['review'].apply(lambda x: x.replace('<br />', ''))\n",
    "print(f\"With no break tags:\\n{df.iloc[1]['review_cleaned'][0:72]}\")\n",
    "\n",
    "#Remove unnecesary whitespace\n",
    "df['review_cleaned'] = df['review_cleaned'].replace(r'\\s+', ' ', regex=True)\n",
    "print(f\"Cleaned:\\n{df.iloc[1]['review_cleaned'][0:72]}\")"
   ],
   "id": "e7eba8d4c59c6df0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "A wonderful little production. <br /><br />The filming technique is very\n",
      "With no break tags:\n",
      "A wonderful little production. The filming technique is very unassuming-\n",
      "Cleaned:\n",
      "A wonderful little production. The filming technique is very unassuming-\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Encoding the sentiment",
   "id": "a8a988ad830eebe1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T12:24:26.693468Z",
     "start_time": "2026-02-09T12:24:26.676983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df['sentiment_encoded'] = df['sentiment'].apply(lambda x:0 if x == 'negative' else 1)\n",
    "df.head(5)"
   ],
   "id": "7dcbc84d4dbdb196",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  One of the other reviewers has mentioned that ...  positive   \n",
       "1  A wonderful little production. <br /><br />The...  positive   \n",
       "2  I thought this was a wonderful way to spend ti...  positive   \n",
       "3  Basically there's a family where a little boy ...  negative   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "\n",
       "                                      review_cleaned  sentiment_encoded  \n",
       "0  One of the other reviewers has mentioned that ...                  1  \n",
       "1  A wonderful little production. The filming tec...                  1  \n",
       "2  I thought this was a wonderful way to spend ti...                  1  \n",
       "3  Basically there's a family where a little boy ...                  0  \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...                  1  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_cleaned</th>\n",
       "      <th>sentiment_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>A wonderful little production. The filming tec...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2 Tokenize the Data",
   "id": "9da3e335f8c0a1b4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T12:24:28.742456Z",
     "start_time": "2026-02-09T12:24:27.385275Z"
    }
   },
   "cell_type": "code",
   "source": "from transformers import BertTokenizer",
   "id": "390c87605bf06e3f",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T12:24:29.723242Z",
     "start_time": "2026-02-09T12:24:28.802254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer"
   ],
   "id": "13938305cace1cc4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Visualization for a single review",
   "id": "4008f59c29cdaeed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T12:24:29.787702Z",
     "start_time": "2026-02-09T12:24:29.779878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Encode a sample sentence\n",
    "sample = \"I liked this movie\"\n",
    "#We can return token_ids tensor in pytorch, numpy or tensor flow format\n",
    "token_ids = tokenizer.encode(sample, return_tensors='np')[0]\n",
    "print(f\"Token IDs:\\n{token_ids}\")\n",
    "\n",
    "#Convert token_ids back to tokens. Just to visualize special tokens added\n",
    "tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "print(f\"Tokens:\\n{tokens}\")"
   ],
   "id": "23656dc9c4d95e8a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      "[ 101 1045 4669 2023 3185  102]\n",
      "Tokens:\n",
      "['[CLS]', 'i', 'liked', 'this', 'movie', '[SEP]']\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T12:24:32.792395Z",
     "start_time": "2026-02-09T12:24:32.784295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "review = df['review_cleaned'].iloc[0]\n",
    "token_ids = tokenizer.encode(review, max_length = 512, padding = 'max_length', truncation = True, return_tensors = 'pt')\n",
    "token_ids #Padding starts at 393th token"
   ],
   "id": "44eb98cae2654d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2028,  1997,  1996,  2060, 15814,  2038,  3855,  2008,  2044,\n",
       "          3666,  2074,  1015, 11472,  2792,  2017,  1005,  2222,  2022, 13322,\n",
       "          1012,  2027,  2024,  2157,  1010,  2004,  2023,  2003,  3599,  2054,\n",
       "          3047,  2007,  2033,  1012,  1996,  2034,  2518,  2008,  4930,  2033,\n",
       "          2055, 11472,  2001,  2049, 24083,  1998,  4895, 10258,  2378,  8450,\n",
       "          5019,  1997,  4808,  1010,  2029,  2275,  1999,  2157,  2013,  1996,\n",
       "          2773,  2175,  1012,  3404,  2033,  1010,  2023,  2003,  2025,  1037,\n",
       "          2265,  2005,  1996,  8143, 18627,  2030,  5199,  3593,  1012,  2023,\n",
       "          2265,  8005,  2053, 17957,  2007, 12362,  2000,  5850,  1010,  3348,\n",
       "          2030,  4808,  1012,  2049,  2003, 13076,  1010,  1999,  1996,  4438,\n",
       "          2224,  1997,  1996,  2773,  1012,  2009,  2003,  2170, 11472,  2004,\n",
       "          2008,  2003,  1996,  8367,  2445,  2000,  1996, 17411,  4555,  3036,\n",
       "          2110,  7279,  4221, 12380,  2854,  1012,  2009,  7679,  3701,  2006,\n",
       "         14110,  2103,  1010,  2019,  6388,  2930,  1997,  1996,  3827,  2073,\n",
       "          2035,  1996,  4442,  2031,  3221, 21430,  1998,  2227, 20546,  2015,\n",
       "          1010,  2061,  9394,  2003,  2025,  2152,  2006,  1996, 11376,  1012,\n",
       "          7861,  2103,  2003,  2188,  2000,  2116,  1012,  1012, 26030,  2015,\n",
       "          1010,  7486,  1010, 18542, 10230,  1010,  7402,  2015,  1010,  8135,\n",
       "          1010, 16773,  1010,  3493,  1998,  2062,  1012,  1012,  1012,  1012,\n",
       "          2061,  8040, 16093, 28331,  1010,  2331, 14020,  1010, 26489,  6292,\n",
       "         24069,  1998, 22824, 10540,  2024,  2196,  2521,  2185,  1012,  1045,\n",
       "          2052,  2360,  1996,  2364,  5574,  1997,  1996,  2265,  2003,  2349,\n",
       "          2000,  1996,  2755,  2008,  2009,  3632,  2073,  2060,  3065,  2876,\n",
       "          1005,  1056,  8108,  1012,  5293,  3492,  4620,  4993,  2005,  7731,\n",
       "          9501,  1010,  5293, 11084,  1010,  5293,  7472,  1012,  1012,  1012,\n",
       "         11472,  2987,  1005,  1056,  6752,  2105,  1012,  1996,  2034,  2792,\n",
       "          1045,  2412,  2387,  4930,  2033,  2004,  2061, 11808,  2009,  2001,\n",
       "         16524,  1010,  1045,  2481,  1005,  1056,  2360,  1045,  2001,  3201,\n",
       "          2005,  2009,  1010,  2021,  2004,  1045,  3427,  2062,  1010,  1045,\n",
       "          2764,  1037,  5510,  2005, 11472,  1010,  1998,  2288, 17730,  2000,\n",
       "          1996,  2152,  3798,  1997,  8425,  4808,  1012,  2025,  2074,  4808,\n",
       "          1010,  2021, 21321,  1006, 15274,  4932,  2040,  1005,  2222,  2022,\n",
       "          2853,  2041,  2005,  1037, 15519,  1010, 13187,  2040,  1005,  2222,\n",
       "          3102,  2006,  2344,  1998,  2131,  2185,  2007,  2009,  1010,  2092,\n",
       "          5450,  2098,  1010,  2690,  2465, 13187,  2108,  2357,  2046,  3827,\n",
       "          7743,  2229,  2349,  2000,  2037,  3768,  1997,  2395,  4813,  2030,\n",
       "          3827,  3325,  1007,  3666, 11472,  1010,  2017,  2089,  2468,  6625,\n",
       "          2007,  2054,  2003,  8796, 10523,  1012,  1012,  1012,  1012,  2008,\n",
       "          2015,  2065,  2017,  2064,  2131,  1999,  3543,  2007,  2115,  9904,\n",
       "          2217,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Nota: tokenizer.encode() puede devolver los token_ids tanto como un tensor de numpy, pytorch or tensorflow. Para este tipo de proyectos se recomienda el formato pytorch ya que esta mejor preparado para usarse en CUDA",
   "id": "b0d59b6bb98516d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T08:28:12.492359Z",
     "start_time": "2026-02-09T08:28:12.098766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "review = df['review_cleaned'].iloc[0]\n",
    "batch_encoder = tokenizer.encode_plus(review, max_length = 512, padding = 'max_length', truncation = True, return_tensors = 'pt')"
   ],
   "id": "2c558d0b928c7fde",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "BertTokenizer has no attribute encode_plus",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[11]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m review = df[\u001B[33m'\u001B[39m\u001B[33mreview_cleaned\u001B[39m\u001B[33m'\u001B[39m].iloc[\u001B[32m0\u001B[39m]\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m batch_encoder = \u001B[43mtokenizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mencode_plus\u001B[49m(review, max_length = \u001B[32m512\u001B[39m, padding = \u001B[33m'\u001B[39m\u001B[33mmax_length\u001B[39m\u001B[33m'\u001B[39m, truncation = \u001B[38;5;28;01mTrue\u001B[39;00m, return_tensors = \u001B[33m'\u001B[39m\u001B[33mpt\u001B[39m\u001B[33m'\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/proyectos/oeg/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1291\u001B[39m, in \u001B[36mPreTrainedTokenizerBase.__getattr__\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m   1288\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.convert_tokens_to_ids(tokens) \u001B[38;5;28;01mif\u001B[39;00m key != key_without_id \u001B[38;5;28;01melse\u001B[39;00m tokens\n\u001B[32m   1290\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.\u001B[34m__dict__\u001B[39m:\n\u001B[32m-> \u001B[39m\u001B[32m1291\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.\u001B[34m__class__\u001B[39m.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m has no attribute \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m   1292\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m().\u001B[34m__getattr__\u001B[39m(key)\n",
      "\u001B[31mAttributeError\u001B[39m: BertTokenizer has no attribute encode_plus"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Nota: En la versión actual de la libreria transformers, ya no se usa encode_plus(). Lo que solia hacer encode_plus() ahora se maneja automaticamente desde la funcion tokenizer()**",
   "id": "cbc9e74b5ea56437"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T12:24:40.121069Z",
     "start_time": "2026-02-09T12:24:40.117581Z"
    }
   },
   "cell_type": "code",
   "source": [
    "review = df['review_cleaned'].iloc[0]\n",
    "batch_encoder = tokenizer(review, max_length = 512, padding = 'max_length', truncation = True, return_tensors = 'pt')"
   ],
   "id": "37967b1f6e9afeab",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T12:24:40.136648Z",
     "start_time": "2026-02-09T12:24:40.129803Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Batch encoder keys:\\n{batch_encoder.keys()}\\n\")",
   "id": "7290236a54689657",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch encoder keys:\n",
      "KeysView({'input_ids': tensor([[  101,  2028,  1997,  1996,  2060, 15814,  2038,  3855,  2008,  2044,\n",
      "          3666,  2074,  1015, 11472,  2792,  2017,  1005,  2222,  2022, 13322,\n",
      "          1012,  2027,  2024,  2157,  1010,  2004,  2023,  2003,  3599,  2054,\n",
      "          3047,  2007,  2033,  1012,  1996,  2034,  2518,  2008,  4930,  2033,\n",
      "          2055, 11472,  2001,  2049, 24083,  1998,  4895, 10258,  2378,  8450,\n",
      "          5019,  1997,  4808,  1010,  2029,  2275,  1999,  2157,  2013,  1996,\n",
      "          2773,  2175,  1012,  3404,  2033,  1010,  2023,  2003,  2025,  1037,\n",
      "          2265,  2005,  1996,  8143, 18627,  2030,  5199,  3593,  1012,  2023,\n",
      "          2265,  8005,  2053, 17957,  2007, 12362,  2000,  5850,  1010,  3348,\n",
      "          2030,  4808,  1012,  2049,  2003, 13076,  1010,  1999,  1996,  4438,\n",
      "          2224,  1997,  1996,  2773,  1012,  2009,  2003,  2170, 11472,  2004,\n",
      "          2008,  2003,  1996,  8367,  2445,  2000,  1996, 17411,  4555,  3036,\n",
      "          2110,  7279,  4221, 12380,  2854,  1012,  2009,  7679,  3701,  2006,\n",
      "         14110,  2103,  1010,  2019,  6388,  2930,  1997,  1996,  3827,  2073,\n",
      "          2035,  1996,  4442,  2031,  3221, 21430,  1998,  2227, 20546,  2015,\n",
      "          1010,  2061,  9394,  2003,  2025,  2152,  2006,  1996, 11376,  1012,\n",
      "          7861,  2103,  2003,  2188,  2000,  2116,  1012,  1012, 26030,  2015,\n",
      "          1010,  7486,  1010, 18542, 10230,  1010,  7402,  2015,  1010,  8135,\n",
      "          1010, 16773,  1010,  3493,  1998,  2062,  1012,  1012,  1012,  1012,\n",
      "          2061,  8040, 16093, 28331,  1010,  2331, 14020,  1010, 26489,  6292,\n",
      "         24069,  1998, 22824, 10540,  2024,  2196,  2521,  2185,  1012,  1045,\n",
      "          2052,  2360,  1996,  2364,  5574,  1997,  1996,  2265,  2003,  2349,\n",
      "          2000,  1996,  2755,  2008,  2009,  3632,  2073,  2060,  3065,  2876,\n",
      "          1005,  1056,  8108,  1012,  5293,  3492,  4620,  4993,  2005,  7731,\n",
      "          9501,  1010,  5293, 11084,  1010,  5293,  7472,  1012,  1012,  1012,\n",
      "         11472,  2987,  1005,  1056,  6752,  2105,  1012,  1996,  2034,  2792,\n",
      "          1045,  2412,  2387,  4930,  2033,  2004,  2061, 11808,  2009,  2001,\n",
      "         16524,  1010,  1045,  2481,  1005,  1056,  2360,  1045,  2001,  3201,\n",
      "          2005,  2009,  1010,  2021,  2004,  1045,  3427,  2062,  1010,  1045,\n",
      "          2764,  1037,  5510,  2005, 11472,  1010,  1998,  2288, 17730,  2000,\n",
      "          1996,  2152,  3798,  1997,  8425,  4808,  1012,  2025,  2074,  4808,\n",
      "          1010,  2021, 21321,  1006, 15274,  4932,  2040,  1005,  2222,  2022,\n",
      "          2853,  2041,  2005,  1037, 15519,  1010, 13187,  2040,  1005,  2222,\n",
      "          3102,  2006,  2344,  1998,  2131,  2185,  2007,  2009,  1010,  2092,\n",
      "          5450,  2098,  1010,  2690,  2465, 13187,  2108,  2357,  2046,  3827,\n",
      "          7743,  2229,  2349,  2000,  2037,  3768,  1997,  2395,  4813,  2030,\n",
      "          3827,  3325,  1007,  3666, 11472,  1010,  2017,  2089,  2468,  6625,\n",
      "          2007,  2054,  2003,  8796, 10523,  1012,  1012,  1012,  1012,  2008,\n",
      "          2015,  2065,  2017,  2064,  2131,  1999,  3543,  2007,  2115,  9904,\n",
      "          2217,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])})\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T12:24:40.183573Z",
     "start_time": "2026-02-09T12:24:40.179182Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Attention mask:\\n{batch_encoder['attention_mask']}\")",
   "id": "21a6bc84f2bb172f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention mask:\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Encoding all reviews",
   "id": "598f6a0ca5606f44"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T12:24:40.237819Z",
     "start_time": "2026-02-09T12:24:40.234285Z"
    }
   },
   "cell_type": "code",
   "source": "import torch",
   "id": "e671ff9de6c31ddd",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T12:25:31.743387Z",
     "start_time": "2026-02-09T12:24:40.284455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "\n",
    "token_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "#Encode each review\n",
    "for review in df['review_cleaned']:\n",
    "    batch_encoder = tokenizer(review, max_length = 512, padding = 'max_length', truncation = True, return_tensors = 'pt')\n",
    "    token_ids.append(batch_encoder['input_ids'])\n",
    "    attention_masks.append(batch_encoder['attention_mask'])\n",
    "\n",
    "#Convert token_ids and attention_masks to pytorch tensors\n",
    "token_ids = torch.cat(token_ids, dim = 0)\n",
    "attention_masks = torch.cat(attention_masks, dim = 0)"
   ],
   "id": "13bc89a266fa67ed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 51.1 s, sys: 420 ms, total: 51.5 s\n",
      "Wall time: 51.5 s\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3 Create the Train and Validation DataLoaders",
   "id": "637eebc984a81076"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T12:25:32.508484Z",
     "start_time": "2026-02-09T12:25:31.807724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ],
   "id": "e1dcc4bca726d536",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T12:25:32.580092Z",
     "start_time": "2026-02-09T12:25:32.514144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val_size = 0.1\n",
    "\n",
    "#Split the token IDs\n",
    "train_ids, val_ids = train_test_split(token_ids, test_size=val_size, shuffle=False)\n",
    "\n",
    "#Split the attention masks\n",
    "train_masks, val_masks = train_test_split(attention_masks, test_size=val_size, shuffle=False)\n",
    "\n",
    "#Split the labels\n",
    "labels = torch.tensor(df['sentiment_encoded'].values)\n",
    "train_labels, val_labels = train_test_split(labels, test_size=val_size, shuffle=False)\n",
    "\n",
    "#Create the DataLoaders\n",
    "train_data = TensorDataset(train_ids, train_masks, train_labels)\n",
    "train_dataloader = DataLoader(train_data, shuffle = True, batch_size = 16)\n",
    "val_data = TensorDataset(val_ids, val_masks, val_labels)\n",
    "val_dataloader = DataLoader(val_data, batch_size = 16)"
   ],
   "id": "3367f0d3857ddbef",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Nota: **Sobre suffle**\n",
    "   - Al hacer las particiones es importante no mezclar los elementos (shuffle=False). De otro modo, perderiamos la asociaciones entre ids, mascaras y labels\n",
    "   - En train_dataloader se tiene suffle=True porque el objetivo del conjunto de entrenamiento es que el modelo aprenda los patrones del texto, e ignore información asociada a las posiciones de los elementos\n",
    "   - En data_loader hacer suffle es innecesario porque el modelo no obtiene información a partir de el, solo se usa para evaluar su rendimiento"
   ],
   "id": "eb971d6fb910789f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4 Instantiate a BERT model",
   "id": "cf3ae23e8d894963"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T12:25:34.672488Z",
     "start_time": "2026-02-09T12:25:32.591800Z"
    }
   },
   "cell_type": "code",
   "source": "from transformers import BertForSequenceClassification",
   "id": "beb9ac962296b83d",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T12:25:35.321349Z",
     "start_time": "2026-02-09T12:25:34.729079Z"
    }
   },
   "cell_type": "code",
   "source": "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 2)",
   "id": "81d332a388285c63",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8ab1a7354f174171b50b7328a6b7afa6"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[1mBertForSequenceClassification LOAD REPORT\u001B[0m from: bert-base-uncased\n",
      "Key                                        | Status     | \n",
      "-------------------------------------------+------------+-\n",
      "cls.seq_relationship.bias                  | UNEXPECTED | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | \n",
      "cls.predictions.bias                       | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED | \n",
      "cls.seq_relationship.weight                | UNEXPECTED | \n",
      "classifier.bias                            | MISSING    | \n",
      "classifier.weight                          | MISSING    | \n",
      "\n",
      "\u001B[3mNotes:\n",
      "- UNEXPECTED\u001B[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001B[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Nota: Aunque parezca un error, este status report es normal. Significa que el modelo aun no ha sido entrenado para nuestra tarea, por lo que los pesos de la capa lineal aun no tienen valor definido",
   "id": "46a8efbc019ef2e8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5 Instantitate an Optimizer, Loss Function and Scheduler",
   "id": "5a516e0dc4d7a736"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "from transformers import get_linear_schedule_with_warmup"
   ],
   "id": "50558cc654feb83e",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "EPOCHS = 2\n",
    "\n",
    "#Optimizer\n",
    "optimizer = AdamW(model.parameters())\n",
    "\n",
    "#Loss function\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "#Scheduler: To gradually reduce the learning rate as the training process continues\n",
    "num_training_steps = EPOCHS * len(train_dataloader)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps = 0, num_training_steps = num_training_steps\n",
    ")"
   ],
   "id": "45e83aff11e8feda",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6 Fine-Tunning Loop",
   "id": "60d30d6e03269c07"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Nota: Para tener mejores tiempos de entrenamiento podemos usar la potencia de la GPU a traves de la plataforma CUDA.\n",
    "- Para poder utilizar CUDA es necesario instalar los drivers de NVIDIA"
   ],
   "id": "b05a236fb4c94f55"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Check if GPUs with CUDA are available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA available\")\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    print(\"CUDA not available\")\n",
    "    device = torch.device('cpu')"
   ],
   "id": "98be0b8d345cbfff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T13:01:21.437276Z",
     "start_time": "2026-02-09T13:01:21.434958Z"
    }
   },
   "cell_type": "code",
   "source": "import numpy as np",
   "id": "67111af6520971aa",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T13:03:16.506629Z",
     "start_time": "2026-02-09T13:03:16.503564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis = 1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    accuracy = np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "    return accuracy"
   ],
   "id": "fc8c872c25670151",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-09T13:05:32.116884Z",
     "start_time": "2026-02-09T13:05:30.858618Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.to(device)\n",
    "\n",
    "for epoch in range(0, EPOCHS):\n",
    "    #1. Switch the model to be in \"Train Mode\". Activates the dropout layer\n",
    "    model.train()\n",
    "\n",
    "    #2. Save training loss to track it over subsequent epochs.\n",
    "    #It should decrease with each epoch if training is successful\n",
    "    training_loss = 0\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "\n",
    "        #3. Move token IDs, attention masks and labels to GPU (if available)\n",
    "        batch_token_ids = batch[0].to(device)\n",
    "        batch_attention_mask = batch[1].to(device)\n",
    "        batch_labels = batch[2].to(device)\n",
    "\n",
    "        #4. Reset the calculated gradients from the previous iteration loop\n",
    "        model.zero_grad()\n",
    "\n",
    "        #5. Pass the batch to the model to calculate the logits (predictions based\n",
    "        # on the current classifier parameters (weigths and biases)) and loss\n",
    "        loss, logits = model (\n",
    "            batch_token_ids,\n",
    "            token_type_ids = None,\n",
    "            attention_mask = batch_attention_mask,\n",
    "            labels = batch_labels,\n",
    "            return_dict = False\n",
    "        )\n",
    "\n",
    "        #6. Extract the total loss for the epoch\n",
    "        #Loss is returned as a PyTorch tensor, .item() extracts its float value\n",
    "        training_loss += loss.item()\n",
    "\n",
    "        #7. Perform a backwar pass of the madel and propagate the loss through the classifier\n",
    "        #head. This will allow the model to adjust its parameters to improve the performance\n",
    "        loss.backward()\n",
    "\n",
    "        #8. Clip the gradient to be no longer than 1.0.\n",
    "        # So the model does not suffer from the exploding gradients problem\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        #9. Call the optimizer to take a step in the direction of the error surface as\n",
    "        #determined by the backward pass\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "    #10. Calculate the average loss and time taken for training on the epoch\n",
    "    average_train_loss = training_loss / len(train_dataloader)\n",
    "\n",
    "    #Validation step for the epoch\n",
    "\n",
    "    #11. Switch the model to \"Evaluation Mode\". Deactivates the dropout layer\n",
    "    model.eval()\n",
    "\n",
    "    #12. Set the validation loss to 0.\n",
    "    val_loss = 0\n",
    "    val_accuracy = 0\n",
    "\n",
    "    #13. Split the validation data into batches (already done)\n",
    "\n",
    "    for batch in val_dataloader:\n",
    "\n",
    "        #14. Move token IDs, attention masks and labels to GPU (if available)\n",
    "        batch_token_ids = batch[0].to(device)\n",
    "        batch_attention_mask = batch[1].to(device)\n",
    "        batch_labels = batch[2].to(device)\n",
    "\n",
    "        #15. Invoke no_grad() to instruct the model not to calculate the gradients\n",
    "        #since we will not be performing any optimization, only inference\n",
    "        with torch.no_grad():\n",
    "            #16. Pass the batch to the model to calculate the logits and the loss\n",
    "            loss, logits = model (\n",
    "                batch_token_ids,\n",
    "                token_type_ids = None,\n",
    "                attention_mask = batch_attention_mask,\n",
    "                labels = batch_labels,\n",
    "                return_dict = False\n",
    "            )\n",
    "\n",
    "        #17. Extract the logits and labels from the model and move them to the CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = batch_labels.to('cpu').numpy()\n",
    "\n",
    "        #18. Increment the loss and calculate the accuracy based on the true labels\n",
    "        #in the validation dataloader\n",
    "        val_loss += loss.item()\n",
    "\n",
    "        #19. Calculate the average loss and accuracy\n",
    "        val_accuracy += calculate_accuracy(logits, label_ids)\n",
    "\n",
    "    average_val_accuracy = val_accuracy / len(val_dataloader)\n"
   ],
   "id": "e08d6c65299cd43c",
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 3.68 GiB of which 79.25 MiB is free. Including non-PyTorch memory, this process has 3.59 GiB memory in use. Of the allocated memory 3.46 GiB is allocated by PyTorch, and 47.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mOutOfMemoryError\u001B[39m                          Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[26]\u001B[39m\u001B[32m, line 23\u001B[39m\n\u001B[32m     19\u001B[39m model.zero_grad()\n\u001B[32m     21\u001B[39m \u001B[38;5;66;03m#5. Pass the batch to the model to calculate the logits (predictions based\u001B[39;00m\n\u001B[32m     22\u001B[39m \u001B[38;5;66;03m# on the current classifier parameters (weigths and biases)) and loss\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m23\u001B[39m loss, logits = \u001B[43mmodel\u001B[49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     24\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbatch_token_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     25\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     26\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     27\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_labels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     28\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m \u001B[49m\u001B[43m=\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\n\u001B[32m     29\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     31\u001B[39m \u001B[38;5;66;03m#6. Extract the total loss for the epoch\u001B[39;00m\n\u001B[32m     32\u001B[39m \u001B[38;5;66;03m#Loss is returned as a PyTorch tensor, .item() extracts its float value\u001B[39;00m\n\u001B[32m     33\u001B[39m training_loss += loss.item()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/proyectos/oeg/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1776\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1774\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1775\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1776\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/proyectos/oeg/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1787\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1784\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1786\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1787\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1789\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1790\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/proyectos/oeg/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:834\u001B[39m, in \u001B[36mcan_return_tuple.<locals>.wrapper\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    832\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m return_dict_passed \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    833\u001B[39m     return_dict = return_dict_passed\n\u001B[32m--> \u001B[39m\u001B[32m834\u001B[39m output = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    835\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m return_dict \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(output, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[32m    836\u001B[39m     output = output.to_tuple()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/proyectos/oeg/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:1138\u001B[39m, in \u001B[36mBertForSequenceClassification.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, **kwargs)\u001B[39m\n\u001B[32m   1120\u001B[39m \u001B[38;5;129m@can_return_tuple\u001B[39m\n\u001B[32m   1121\u001B[39m \u001B[38;5;129m@auto_docstring\u001B[39m\n\u001B[32m   1122\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\n\u001B[32m   (...)\u001B[39m\u001B[32m   1130\u001B[39m     **kwargs: Unpack[TransformersKwargs],\n\u001B[32m   1131\u001B[39m ) -> \u001B[38;5;28mtuple\u001B[39m[torch.Tensor] | SequenceClassifierOutput:\n\u001B[32m   1132\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33mr\u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1133\u001B[39m \u001B[33;03m    labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001B[39;00m\n\u001B[32m   1134\u001B[39m \u001B[33;03m        Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001B[39;00m\n\u001B[32m   1135\u001B[39m \u001B[33;03m        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001B[39;00m\n\u001B[32m   1136\u001B[39m \u001B[33;03m        `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001B[39;00m\n\u001B[32m   1137\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m1138\u001B[39m     outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1139\u001B[39m \u001B[43m        \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1140\u001B[39m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1141\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtoken_type_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1142\u001B[39m \u001B[43m        \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1143\u001B[39m \u001B[43m        \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1144\u001B[39m \u001B[43m        \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m   1145\u001B[39m \u001B[43m        \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1146\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1148\u001B[39m     pooled_output = outputs[\u001B[32m1\u001B[39m]\n\u001B[32m   1150\u001B[39m     pooled_output = \u001B[38;5;28mself\u001B[39m.dropout(pooled_output)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/proyectos/oeg/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1776\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1774\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1775\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1776\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/proyectos/oeg/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1787\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1784\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1786\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1787\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1789\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1790\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/proyectos/oeg/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:1001\u001B[39m, in \u001B[36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    999\u001B[39m             outputs = func(\u001B[38;5;28mself\u001B[39m, *args, **kwargs)\n\u001B[32m   1000\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1001\u001B[39m         outputs = \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1002\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m original_exception:\n\u001B[32m   1003\u001B[39m     \u001B[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001B[39;00m\n\u001B[32m   1004\u001B[39m     \u001B[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001B[39;00m\n\u001B[32m   1005\u001B[39m     \u001B[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001B[39;00m\n\u001B[32m   1006\u001B[39m     kwargs_without_recordable = {k: v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m kwargs.items() \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m recordable_keys}\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/proyectos/oeg/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:696\u001B[39m, in \u001B[36mBertModel.forward\u001B[39m\u001B[34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, cache_position, **kwargs)\u001B[39m\n\u001B[32m    679\u001B[39m embedding_output = \u001B[38;5;28mself\u001B[39m.embeddings(\n\u001B[32m    680\u001B[39m     input_ids=input_ids,\n\u001B[32m    681\u001B[39m     position_ids=position_ids,\n\u001B[32m   (...)\u001B[39m\u001B[32m    684\u001B[39m     past_key_values_length=past_key_values_length,\n\u001B[32m    685\u001B[39m )\n\u001B[32m    687\u001B[39m attention_mask, encoder_attention_mask = \u001B[38;5;28mself\u001B[39m._create_attention_masks(\n\u001B[32m    688\u001B[39m     attention_mask=attention_mask,\n\u001B[32m    689\u001B[39m     encoder_attention_mask=encoder_attention_mask,\n\u001B[32m   (...)\u001B[39m\u001B[32m    693\u001B[39m     past_key_values=past_key_values,\n\u001B[32m    694\u001B[39m )\n\u001B[32m--> \u001B[39m\u001B[32m696\u001B[39m encoder_outputs = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    697\u001B[39m \u001B[43m    \u001B[49m\u001B[43membedding_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    698\u001B[39m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    699\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    700\u001B[39m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    701\u001B[39m \u001B[43m    \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    702\u001B[39m \u001B[43m    \u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m=\u001B[49m\u001B[43muse_cache\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    703\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    704\u001B[39m \u001B[43m    \u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m=\u001B[49m\u001B[43mposition_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    705\u001B[39m \u001B[43m    \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    706\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    707\u001B[39m sequence_output = encoder_outputs.last_hidden_state\n\u001B[32m    708\u001B[39m pooled_output = \u001B[38;5;28mself\u001B[39m.pooler(sequence_output) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.pooler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/proyectos/oeg/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1776\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1774\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1775\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1776\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/proyectos/oeg/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1787\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1784\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1786\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1787\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1789\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1790\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/proyectos/oeg/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:452\u001B[39m, in \u001B[36mBertEncoder.forward\u001B[39m\u001B[34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, cache_position, **kwargs)\u001B[39m\n\u001B[32m    440\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\n\u001B[32m    441\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    442\u001B[39m     hidden_states: torch.Tensor,\n\u001B[32m   (...)\u001B[39m\u001B[32m    449\u001B[39m     **kwargs: Unpack[TransformersKwargs],\n\u001B[32m    450\u001B[39m ) -> \u001B[38;5;28mtuple\u001B[39m[torch.Tensor] | BaseModelOutputWithPastAndCrossAttentions:\n\u001B[32m    451\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m i, layer_module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m.layer):\n\u001B[32m--> \u001B[39m\u001B[32m452\u001B[39m         hidden_states = \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    453\u001B[39m \u001B[43m            \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    454\u001B[39m \u001B[43m            \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    455\u001B[39m \u001B[43m            \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001B[39;49;00m\n\u001B[32m    456\u001B[39m \u001B[43m            \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m=\u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    457\u001B[39m \u001B[43m            \u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m=\u001B[49m\u001B[43mpast_key_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    458\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcache_position\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    459\u001B[39m \u001B[43m            \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    460\u001B[39m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    462\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m BaseModelOutputWithPastAndCrossAttentions(\n\u001B[32m    463\u001B[39m         last_hidden_state=hidden_states,\n\u001B[32m    464\u001B[39m         past_key_values=past_key_values \u001B[38;5;28;01mif\u001B[39;00m use_cache \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m    465\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/proyectos/oeg/.venv/lib/python3.12/site-packages/transformers/modeling_layers.py:93\u001B[39m, in \u001B[36mGradientCheckpointingLayer.__call__\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m     90\u001B[39m         logger.warning_once(message)\n\u001B[32m     92\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._gradient_checkpointing_func(partial(\u001B[38;5;28msuper\u001B[39m().\u001B[34m__call__\u001B[39m, **kwargs), *args)\n\u001B[32m---> \u001B[39m\u001B[32m93\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[34;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/proyectos/oeg/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1776\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1774\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1775\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1776\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/proyectos/oeg/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1787\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1784\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1786\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1787\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1789\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1790\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/proyectos/oeg/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:423\u001B[39m, in \u001B[36mBertLayer.forward\u001B[39m\u001B[34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, cache_position, **kwargs)\u001B[39m\n\u001B[32m    413\u001B[39m     cross_attention_output, _ = \u001B[38;5;28mself\u001B[39m.crossattention(\n\u001B[32m    414\u001B[39m         self_attention_output,\n\u001B[32m    415\u001B[39m         \u001B[38;5;28;01mNone\u001B[39;00m,  \u001B[38;5;66;03m# attention_mask\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    419\u001B[39m         **kwargs,\n\u001B[32m    420\u001B[39m     )\n\u001B[32m    421\u001B[39m     attention_output = cross_attention_output\n\u001B[32m--> \u001B[39m\u001B[32m423\u001B[39m layer_output = \u001B[43mapply_chunking_to_forward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    424\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfeed_forward_chunk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mchunk_size_feed_forward\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mseq_len_dim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattention_output\u001B[49m\n\u001B[32m    425\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    426\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m layer_output\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/proyectos/oeg/.venv/lib/python3.12/site-packages/transformers/pytorch_utils.py:201\u001B[39m, in \u001B[36mapply_chunking_to_forward\u001B[39m\u001B[34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001B[39m\n\u001B[32m    198\u001B[39m     \u001B[38;5;66;03m# concatenate output at same dimension\u001B[39;00m\n\u001B[32m    199\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m torch.cat(output_chunks, dim=chunk_dim)\n\u001B[32m--> \u001B[39m\u001B[32m201\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43minput_tensors\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/proyectos/oeg/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:429\u001B[39m, in \u001B[36mBertLayer.feed_forward_chunk\u001B[39m\u001B[34m(self, attention_output)\u001B[39m\n\u001B[32m    428\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mfeed_forward_chunk\u001B[39m(\u001B[38;5;28mself\u001B[39m, attention_output):\n\u001B[32m--> \u001B[39m\u001B[32m429\u001B[39m     intermediate_output = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mintermediate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mattention_output\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    430\u001B[39m     layer_output = \u001B[38;5;28mself\u001B[39m.output(intermediate_output, attention_output)\n\u001B[32m    431\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m layer_output\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/proyectos/oeg/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1776\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1774\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1775\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1776\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/proyectos/oeg/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1787\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1784\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1786\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1787\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1789\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1790\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/proyectos/oeg/.venv/lib/python3.12/site-packages/transformers/models/bert/modeling_bert.py:348\u001B[39m, in \u001B[36mBertIntermediate.forward\u001B[39m\u001B[34m(self, hidden_states)\u001B[39m\n\u001B[32m    347\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, hidden_states: torch.Tensor) -> torch.Tensor:\n\u001B[32m--> \u001B[39m\u001B[32m348\u001B[39m     hidden_states = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdense\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    349\u001B[39m     hidden_states = \u001B[38;5;28mself\u001B[39m.intermediate_act_fn(hidden_states)\n\u001B[32m    350\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m hidden_states\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/proyectos/oeg/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1776\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1774\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1775\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1776\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/proyectos/oeg/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1787\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1782\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1783\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1784\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1785\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1786\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1787\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1789\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1790\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/proyectos/oeg/.venv/lib/python3.12/site-packages/torch/nn/modules/linear.py:134\u001B[39m, in \u001B[36mLinear.forward\u001B[39m\u001B[34m(self, input)\u001B[39m\n\u001B[32m    130\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) -> Tensor:\n\u001B[32m    131\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    132\u001B[39m \u001B[33;03m    Runs the forward pass.\u001B[39;00m\n\u001B[32m    133\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m134\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mOutOfMemoryError\u001B[39m: CUDA out of memory. Tried to allocate 96.00 MiB. GPU 0 has a total capacity of 3.68 GiB of which 79.25 MiB is free. Including non-PyTorch memory, this process has 3.59 GiB memory in use. Of the allocated memory 3.46 GiB is allocated by PyTorch, and 47.88 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Nota: **Sobre CUDA y GPU**\n",
    "Para poder realizar los entrenamientos, todos los elementos incluido el propio modelo deben estar en el mismo sitio, idealmente la GPU. Por ello es necesario incluir los llamadas a .to(device)"
   ],
   "id": "4ae662ea08cf125a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Nota:\n",
    "Con los hiperparametros actuales, batch_size=32 en los DataLoaders y max_length=512 en Encoding, mi ordenador (RTX 3050 con 4GB VRAM) no es capaz de entrenar el modelo por falta de memoria. Se estima que se necesita alrededor de 12GB VRAM. Para poder completar el entrenamiento debo reducir los hiperparametros o entrenar un modelo más pequeño."
   ],
   "id": "4e3eda1580be4151"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
