{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1 Normalizers",
   "id": "5d99467750f3827c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T13:06:21.211287Z",
     "start_time": "2026-02-06T13:06:21.203086Z"
    }
   },
   "cell_type": "code",
   "source": "from tokenizers.normalizers import Lowercase, NFC, BertNormalizer",
   "id": "c301d797fc66b558",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T13:09:14.968391Z",
     "start_time": "2026-02-06T13:09:14.963244Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Sample text\n",
    "text = \"EsTá ORÄCIÖN Sé DèBÈ nOŔmALÏZÁR\"\n",
    "chinese_text = \"COŃ CARáctEŔès chINös 手田水口廿卜山\""
   ],
   "id": "efbee0c6cd8db825",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T13:09:16.670355Z",
     "start_time": "2026-02-06T13:09:16.666043Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Create normalizers objects\n",
    "lowercase_norm = Lowercase()\n",
    "nfc_norm = NFC()\n",
    "bert_norm = BertNormalizer()"
   ],
   "id": "e167d4f51c947a5f",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T13:09:20.580308Z",
     "start_time": "2026-02-06T13:09:20.574330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Lowercase normalizer:\\n{lowercase_norm.normalize_str(text)}\")\n",
    "print(f\"NFC normalizer:\\n{nfc_norm.normalize_str(text)}\")\n",
    "print(f\"BERT normalizer:\\n{bert_norm.normalize_str(text)}\")"
   ],
   "id": "a6f9e5cea2856e9f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercase normalizer:\n",
      "está oräciön sé dèbè noŕmalïzár\n",
      "NFC normalizer:\n",
      "EsTá ORÄCIÖN Sé DèBÈ nOŔmALÏZÁR\n",
      "BERT normalizer:\n",
      "esta oracion se debe normalizar\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-06T13:09:39.587120Z",
     "start_time": "2026-02-06T13:09:39.581455Z"
    }
   },
   "source": "print(f\"BERT normalizer with chinese chars:\\n{bert_norm.normalize_str(chinese_text)}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERT normalizer with chinese chars:\n",
      "con caracteres chinos  手  田  水  口  廿  卜  山 \n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2 Pre-Tokenization",
   "id": "77b915ad6e2cad3e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T13:06:22.819231Z",
     "start_time": "2026-02-06T13:06:22.815070Z"
    }
   },
   "cell_type": "code",
   "source": "from tokenizers.pre_tokenizers import WhitespaceSplit, BertPreTokenizer",
   "id": "7c162f30265c387c",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T13:35:42.784654Z",
     "start_time": "2026-02-06T13:35:42.782480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = (\"Este texto contiene comas, puntos y comas; parentesís () \" \\\n",
    "        \"y puntos. También apóstrofes Mickey's\")"
   ],
   "id": "5566e65755334d8",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T13:35:44.079867Z",
     "start_time": "2026-02-06T13:35:44.075299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Create pre-tokenizer objects\n",
    "whitespace_split = WhitespaceSplit()\n",
    "bert_pretokenizer = BertPreTokenizer()"
   ],
   "id": "d80890b5e5e6b1c4",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T13:35:45.000620Z",
     "start_time": "2026-02-06T13:35:44.997057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Whitespace split pretokenizer:\")\n",
    "whitespace_split.pre_tokenize_str(text)"
   ],
   "id": "38e664f683aa0b06",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitespace split pretokenizer:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Este', (0, 4)),\n",
       " ('texto', (5, 10)),\n",
       " ('contiene', (11, 19)),\n",
       " ('comas,', (20, 26)),\n",
       " ('puntos', (27, 33)),\n",
       " ('y', (34, 35)),\n",
       " ('comas;', (36, 42)),\n",
       " ('parentesís', (43, 53)),\n",
       " ('()', (54, 56)),\n",
       " ('y', (57, 58)),\n",
       " ('puntos.', (59, 66)),\n",
       " ('También', (67, 74)),\n",
       " ('apóstrofes', (75, 85)),\n",
       " (\"Mickey's\", (86, 94))]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Nota: La tupla que se genera al lado del token corresponde a las posiciones de inicio y final (no inclusivo) del token dentro del texto fuente",
   "id": "d9cbe36111525f75"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T13:35:47.239901Z",
     "start_time": "2026-02-06T13:35:47.233435Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Bert pre tokenizer:\")\n",
    "bert_pretokenizer.pre_tokenize_str(text)"
   ],
   "id": "7ea121270507c6d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bert pre tokenizer:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Este', (0, 4)),\n",
       " ('texto', (5, 10)),\n",
       " ('contiene', (11, 19)),\n",
       " ('comas', (20, 25)),\n",
       " (',', (25, 26)),\n",
       " ('puntos', (27, 33)),\n",
       " ('y', (34, 35)),\n",
       " ('comas', (36, 41)),\n",
       " (';', (41, 42)),\n",
       " ('parentesís', (43, 53)),\n",
       " ('(', (54, 55)),\n",
       " (')', (55, 56)),\n",
       " ('y', (57, 58)),\n",
       " ('puntos', (59, 65)),\n",
       " ('.', (65, 66)),\n",
       " ('También', (67, 74)),\n",
       " ('apóstrofes', (75, 85)),\n",
       " ('Mickey', (86, 92)),\n",
       " (\"'\", (92, 93)),\n",
       " ('s', (93, 94))]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T13:35:51.338223Z",
     "start_time": "2026-02-06T13:35:51.335436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch import * #Transformers has dependencies with pytorch\n",
    "from transformers import AutoTokenizer"
   ],
   "id": "3f628eb39c6f4601",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T13:33:22.633791Z",
     "start_time": "2026-02-06T13:33:17.653926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Pre-tokenizers from common models: GPT2 and ALBERT (A LiteBert)\n",
    "GPT2_pretokenizer = AutoTokenizer.from_pretrained('gpt2').backend_tokenizer.pre_tokenizer\n",
    "ALBERT_pretokenizer = AutoTokenizer.from_pretrained('albert-base-v1').backend_tokenizer.pre_tokenizer"
   ],
   "id": "273972419b5af492",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T13:35:55.382433Z",
     "start_time": "2026-02-06T13:35:55.377082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('GPT2 pre tokenizer:')\n",
    "GPT2_pretokenizer.pre_tokenize_str(text)"
   ],
   "id": "de5d48ca76b22405",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2 pre tokenizer:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Este', (0, 4)),\n",
       " ('Ġtexto', (4, 10)),\n",
       " ('Ġcontiene', (10, 19)),\n",
       " ('Ġcomas', (19, 25)),\n",
       " (',', (25, 26)),\n",
       " ('Ġpuntos', (26, 33)),\n",
       " ('Ġy', (33, 35)),\n",
       " ('Ġcomas', (35, 41)),\n",
       " (';', (41, 42)),\n",
       " ('ĠparentesÃŃs', (42, 53)),\n",
       " ('Ġ()', (53, 56)),\n",
       " ('Ġy', (56, 58)),\n",
       " ('Ġpuntos', (58, 65)),\n",
       " ('.', (65, 66)),\n",
       " ('ĠTambiÃ©n', (66, 74)),\n",
       " ('ĠapÃ³strofes', (74, 85)),\n",
       " ('ĠMickey', (85, 92)),\n",
       " (\"'s\", (92, 94))]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Nota: GPT2 representa los whitespaces con una variante de G especial",
   "id": "9c18a1aebb6ff69a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T13:35:59.180309Z",
     "start_time": "2026-02-06T13:35:59.174993Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('ALBERT pre tokenizer:')\n",
    "ALBERT_pretokenizer.pre_tokenize_str(text)"
   ],
   "id": "857d98e941c410fd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALBERT pre tokenizer:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('▁Este', (0, 4)),\n",
       " ('▁texto', (5, 10)),\n",
       " ('▁contiene', (11, 19)),\n",
       " ('▁comas,', (20, 26)),\n",
       " ('▁puntos', (27, 33)),\n",
       " ('▁y', (34, 35)),\n",
       " ('▁comas;', (36, 42)),\n",
       " ('▁parentesís', (43, 53)),\n",
       " ('▁()', (54, 56)),\n",
       " ('▁y', (57, 58)),\n",
       " ('▁puntos.', (59, 66)),\n",
       " ('▁También', (67, 74)),\n",
       " ('▁apóstrofes', (75, 85)),\n",
       " (\"▁Mickey's\", (86, 94))]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Nota: Bert representa los whitespaces con guión bajo _",
   "id": "5e73ddeb115fb17d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3 Tokenization",
   "id": "97de230a124177a8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T13:56:10.575612Z",
     "start_time": "2026-02-06T13:56:10.573646Z"
    }
   },
   "cell_type": "code",
   "source": "from tokenizers import Tokenizer",
   "id": "cc29925123bc613b",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T14:03:33.400148Z",
     "start_time": "2026-02-06T14:03:33.395267Z"
    }
   },
   "cell_type": "code",
   "source": "text = \"Texto normalizado con el que podemos trabajar\"",
   "id": "66d5c120f224b68",
   "outputs": [],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T14:12:28.268048Z",
     "start_time": "2026-02-06T14:12:28.028990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = Tokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer.post_processor = None #\"Disable\" post-processing"
   ],
   "id": "aba0dde4e0accbdc",
   "outputs": [],
   "execution_count": 85
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T14:12:29.297434Z",
     "start_time": "2026-02-06T14:12:29.293469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output = tokenizer.encode(text)\n",
    "print(\"Tokens: \")\n",
    "list(zip(output.tokens, output.ids))"
   ],
   "id": "1dde384dce5cd1f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('text', 3793),\n",
       " ('##o', 2080),\n",
       " ('normal', 3671),\n",
       " ('##iza', 21335),\n",
       " ('##do', 3527),\n",
       " ('con', 9530),\n",
       " ('el', 3449),\n",
       " ('que', 10861),\n",
       " ('pod', 17491),\n",
       " ('##em', 6633),\n",
       " ('##os', 2891),\n",
       " ('tr', 19817),\n",
       " ('##aba', 19736),\n",
       " ('##jar', 16084)]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 86
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4 Post-processing",
   "id": "137e0e0291c8344e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T14:10:37.135695Z",
     "start_time": "2026-02-06T14:10:37.131766Z"
    }
   },
   "cell_type": "code",
   "source": "from tokenizers.processors import BertProcessing",
   "id": "8cc744451a0912c6",
   "outputs": [],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T14:10:38.018551Z",
     "start_time": "2026-02-06T14:10:37.858906Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bert_processor = BertProcessing(sep = (\"[SEP]\", 1), cls =(\"[CLS]\", 2))\n",
    "\n",
    "tokenizer = Tokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer.post_processor = bert_processor"
   ],
   "id": "f95483fe4f34455b",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Nota: Bert agrega los tokens [SEP] y [CLS] como parte de su post-procesado",
   "id": "a21a77a01076667"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T14:10:57.351097Z",
     "start_time": "2026-02-06T14:10:57.347829Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output = tokenizer.encode(text)\n",
    "\n",
    "print(\"Tokens: \")\n",
    "list(zip(output.tokens, output.ids))"
   ],
   "id": "220f26d24df961cf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('[CLS]', 2),\n",
       " ('text', 3793),\n",
       " ('##o', 2080),\n",
       " ('normal', 3671),\n",
       " ('##iza', 21335),\n",
       " ('##do', 3527),\n",
       " ('con', 9530),\n",
       " ('el', 3449),\n",
       " ('que', 10861),\n",
       " ('pod', 17491),\n",
       " ('##em', 6633),\n",
       " ('##os', 2891),\n",
       " ('tr', 19817),\n",
       " ('##aba', 19736),\n",
       " ('##jar', 16084),\n",
       " ('[SEP]', 1)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 84
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
