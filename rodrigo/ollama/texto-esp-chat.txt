Este año, vimos una sorprendente aplicación del aprendizaje automático. La OpenAI GPT-2 exhibió una capacidad impresionante de escribir ensayos coherentes y apasionados que superan lo que anticipamos los modelos de lenguaje actuales podrían producir. La GPT-2 no fue particularmente un modelo arquitectónico innovador – su arquitectura se parece mucho a el arquitectura del decoder-only transformer. Sin embargo, la GPT2 era un modelo de lenguaje basado en transformadores muy grande, entrenada en una enorme base de datos. En este post, exploraremos la arquitectura que permitió al modelo producir sus resultados. Vamos a sumergirnos en las capas de atención autónoma del modelo y luego examinaremos aplicaciones para el decoder-only transformer más allá del modelado lingüístico.