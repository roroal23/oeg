Este año vimos una impresionante aplicación del aprendizaje automático de máquina. La OpenAI GPT-2 exhibió una capacidad sobresaliente para escribir ensayos coherentes y apasionados que superan lo que habíamos anticipado que pudieran producir los modelos de lenguaje actuales. El GPT-2 no fue particularmente una arquitectura innovadora – su arquitectura es muy similar a el decoder-only transformer. El GPT2, sin embargo, fue un modelo de lenguaje basado en transformadores muy grande, entrenado en un conjunto masivo de datos. En este post, exploraremos la arquitectura que permitió al modelo producir sus resultados. Entraremos en los profundos de su capa de atención al mismo y luego examinaremos aplicaciones para el decoder-only transformer más allá del modelado de lenguaje.