{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "01_how-to-train.ipynb",
   "provenance": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "a58a66392b644b1384661e850c077a6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_view_name": "HBoxView",
      "_dom_classes": [],
      "_model_name": "HBoxModel",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "box_style": "",
      "layout": "IPY_MODEL_a491e8caa0a048beb3b5259f14eb233f",
      "_model_module": "@jupyter-widgets/controls",
      "children": [
       "IPY_MODEL_837c9ddc3d594e088891874560c646b8",
       "IPY_MODEL_dbf50873d62c4ba39321faefbed0cca5"
      ]
     },
     "model_module_version": "1.5.0"
    },
    "a491e8caa0a048beb3b5259f14eb233f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     },
     "model_module_version": "1.2.0"
    },
    "837c9ddc3d594e088891874560c646b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_view_name": "ProgressView",
      "style": "IPY_MODEL_40bf955ba0284e84b198da6be8654219",
      "_dom_classes": [],
      "description": "Epoch: 100%",
      "_model_name": "FloatProgressModel",
      "bar_style": "success",
      "max": 1,
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": 1,
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "orientation": "horizontal",
      "min": 0,
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_fe20a8dae6e84628b5076d02183090f5"
     },
     "model_module_version": "1.5.0"
    },
    "dbf50873d62c4ba39321faefbed0cca5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_view_name": "HTMLView",
      "style": "IPY_MODEL_93b3f9eae3cb4e3e859cf456e3547c6d",
      "_dom_classes": [],
      "description": "",
      "_model_name": "HTMLModel",
      "placeholder": "‚Äã",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": " 1/1 [2:46:46&lt;00:00, 10006.17s/it]",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_6feb10aeb43147e6aba028d065947ae8"
     },
     "model_module_version": "1.5.0"
    },
    "40bf955ba0284e84b198da6be8654219": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "ProgressStyleModel",
      "description_width": "initial",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "bar_color": null,
      "_model_module": "@jupyter-widgets/controls"
     },
     "model_module_version": "1.5.0"
    },
    "fe20a8dae6e84628b5076d02183090f5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     },
     "model_module_version": "1.2.0"
    },
    "93b3f9eae3cb4e3e859cf456e3547c6d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "DescriptionStyleModel",
      "description_width": "",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "_model_module": "@jupyter-widgets/controls"
     },
     "model_module_version": "1.5.0"
    },
    "6feb10aeb43147e6aba028d065947ae8": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     },
     "model_module_version": "1.2.0"
    },
    "0989d41a4da24e9ebff377e02127642c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_view_name": "HBoxView",
      "_dom_classes": [],
      "_model_name": "HBoxModel",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "box_style": "",
      "layout": "IPY_MODEL_42c6061ef7e44f179db5a6e3551c0f17",
      "_model_module": "@jupyter-widgets/controls",
      "children": [
       "IPY_MODEL_d295dd80550447d88da0f04ce36a22ff",
       "IPY_MODEL_04e7e6d291da49d5816dc98a2904e95c"
      ]
     },
     "model_module_version": "1.5.0"
    },
    "42c6061ef7e44f179db5a6e3551c0f17": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     },
     "model_module_version": "1.2.0"
    },
    "d295dd80550447d88da0f04ce36a22ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_view_name": "ProgressView",
      "style": "IPY_MODEL_e7d8c3a4fecd40778e32966b29ea65a1",
      "_dom_classes": [],
      "description": "Iteration: 100%",
      "_model_name": "FloatProgressModel",
      "bar_style": "success",
      "max": 15228,
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": 15228,
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "orientation": "horizontal",
      "min": 0,
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_016d7c8318f742c1943464b08232a510"
     },
     "model_module_version": "1.5.0"
    },
    "04e7e6d291da49d5816dc98a2904e95c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_view_name": "HTMLView",
      "style": "IPY_MODEL_8388e9da9da4492c98c19235ca5fc1b5",
      "_dom_classes": [],
      "description": "",
      "_model_name": "HTMLModel",
      "placeholder": "‚Äã",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": " 15228/15228 [2:46:46&lt;00:00,  1.52it/s]",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_39c23c6a972b419eb2eeeebafeaedc22"
     },
     "model_module_version": "1.5.0"
    },
    "e7d8c3a4fecd40778e32966b29ea65a1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "ProgressStyleModel",
      "description_width": "initial",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "bar_color": null,
      "_model_module": "@jupyter-widgets/controls"
     },
     "model_module_version": "1.5.0"
    },
    "016d7c8318f742c1943464b08232a510": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     },
     "model_module_version": "1.2.0"
    },
    "8388e9da9da4492c98c19235ca5fc1b5": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "DescriptionStyleModel",
      "description_width": "",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "_model_module": "@jupyter-widgets/controls"
     },
     "model_module_version": "1.5.0"
    },
    "39c23c6a972b419eb2eeeebafeaedc22": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     },
     "model_module_version": "1.2.0"
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M1oqh0F6W3ad"
   },
   "source": [
    "# How to train a new language model from scratch using Transformers and Tokenizers\n",
    "\n",
    "### Notebook edition (link to blogpost [link](https://huggingface.co/blog/how-to-train)). Last update May 15, 2020\n",
    "\n",
    "\n",
    "Over the past few months, we made several improvements to our [`transformers`](https://github.com/huggingface/transformers) and [`tokenizers`](https://github.com/huggingface/tokenizers) libraries, with the goal of making it easier than ever to **train a new language model from scratch**.\n",
    "\n",
    "In this post we‚Äôll demo how to train a ‚Äúsmall‚Äù model (84 M parameters = 6 layers, 768 hidden size, 12 attention heads) ‚Äì that‚Äôs the same number of layers & heads as DistilBERT ‚Äì on **Esperanto**. We‚Äôll then fine-tune the model on a downstream task of part-of-speech tagging.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oK7PPVm2XBgr"
   },
   "source": [
    "## 1. Find a dataset\n",
    "\n",
    "First, let us find a corpus of text in Esperanto. Here we‚Äôll use the Esperanto portion of the [OSCAR corpus](https://traces1.inria.fr/oscar/) from INRIA.\n",
    "OSCAR is a huge multilingual corpus obtained by language classification and filtering of [Common Crawl](https://commoncrawl.org/) dumps of the Web.\n",
    "\n",
    "<img src=\"https://huggingface.co/blog/assets/01_how-to-train/oscar.png\" style=\"margin: auto; display: block; width: 260px;\">\n",
    "\n",
    "The Esperanto portion of the dataset is only 299M, so we‚Äôll concatenate with the Esperanto sub-corpus of the [Leipzig Corpora Collection](https://wortschatz.uni-leipzig.de/en/download), which is comprised of text from diverse sources like news, literature, and wikipedia.\n",
    "\n",
    "The final training corpus has a size of 3 GB, which is still small ‚Äì for your model, you will get better results the more data you can get to pretrain on.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HOk4iZ9YZvec",
    "ExecuteTime": {
     "end_time": "2026-02-13T20:32:52.780069908Z",
     "start_time": "2026-02-13T20:32:49.351926698Z"
    }
   },
   "source": [
    "# in this notebook we'll only get one of the files (the Oscar one) for the sake of simplicity and performance\n",
    "!wget -c https://cdn-datasets.huggingface.co/EsperBERTo/data/oscar.eo.txt"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-02-13 21:32:49--  https://cdn-datasets.huggingface.co/EsperBERTo/data/oscar.eo.txt\r\n",
      "Resolving cdn-datasets.huggingface.co (cdn-datasets.huggingface.co)... 3.174.180.16, 3.174.180.105, 3.174.180.76, ...\r\n",
      "Connecting to cdn-datasets.huggingface.co (cdn-datasets.huggingface.co)|3.174.180.16|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 312733741 (298M) [text/plain]\r\n",
      "Saving to: ‚Äòoscar.eo.txt‚Äô\r\n",
      "\r\n",
      "oscar.eo.txt        100%[===================>] 298,25M   112MB/s    in 2,7s    \r\n",
      "\r\n",
      "2026-02-13 21:32:52 (112 MB/s) - ‚Äòoscar.eo.txt‚Äô saved [312733741/312733741]\r\n",
      "\r\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T20:32:58.109482498Z",
     "start_time": "2026-02-13T20:32:57.761201476Z"
    }
   },
   "cell_type": "code",
   "source": [
    "!mkdir data\n",
    "!mv oscar.eo.txt data/"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‚Äòdata‚Äô: File exists\r\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-kkz81OY6xH"
   },
   "source": [
    "## 2. Train a tokenizer\n",
    "\n",
    "We choose to train a byte-level Byte-pair encoding tokenizer (the same as GPT-2), with the same special tokens as RoBERTa. Let‚Äôs arbitrarily pick its size to be 52,000.\n",
    "\n",
    "We recommend training a byte-level BPE (rather than let‚Äôs say, a WordPiece tokenizer like BERT) because it will start building its vocabulary from an alphabet of single bytes, so all words will be decomposable into tokens (no more `<unk>` tokens!).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5duRggBRZKvP"
   },
   "source": [
    "# We won't need TensorFlow here\n",
    "# !pip uninstall -y tensorflow\n",
    "# Install `transformers` from master\n",
    "# !pip install git+https://github.com/huggingface/transformers\n",
    "# !pip list | grep -E 'transformers|tokenizers'\n",
    "# transformers version at notebook update --- 2.11.0\n",
    "# tokenizers version at notebook update --- 0.8.0rc1"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Nota: **Explicacion codigo**\n",
    "\n",
    "paths = [str(x) for x in Path(\".\").glob(\"**/*.txt\")]\n",
    "- Path.glob se encarga de buscar todos los ficheros que calzan un patr√≥n (en este caso cualquiera con extensi√≥n .txt) a partir de la posici√≥n actual. Explora toda el subarbol.\n",
    "- str(x) convierte el objeto Path en la ruta asociada\n",
    "De modo que esta linea busca todas los ficheros acabados en txt y devuelve sus rutas. Para este ejercicio el unico que encontrara sera ./data/oscar.eo.txt"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IMnymRDLe0hi",
    "outputId": "4d26476f-e6b5-475a-a0c1-41b6fcdc041a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "ExecuteTime": {
     "end_time": "2026-02-12T16:13:45.132192Z",
     "start_time": "2026-02-12T16:13:19.449621Z"
    }
   },
   "source": [
    "%%time\n",
    "from pathlib import Path\n",
    "\n",
    "from tokenizers import ByteLevelBPETokenizer\n",
    "\n",
    "paths = [str(x) for x in Path(\".\").glob(\"**/*.txt\")]\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "# Customize training\n",
    "tokenizer.train(files=paths, vocab_size=52_000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "CPU times: user 2min 14s, sys: 5.74 s, total: 2min 20s\n",
      "Wall time: 25.7 s\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Ei7bqpRf1LH"
   },
   "source": [
    "Now let's save files to disk"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EIS-irI0f32P",
    "outputId": "e86c4a24-eb65-4f0a-aa58-ed1931a05ac9",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "ExecuteTime": {
     "end_time": "2026-02-12T15:14:20.551790Z",
     "start_time": "2026-02-12T15:14:20.253683Z"
    }
   },
   "source": [
    "!mkdir EsperBERTo\n",
    "tokenizer.save_model(\"EsperBERTo\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8)\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['EsperBERTo/vocab.json', 'EsperBERTo/merges.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOOfYSuQhSqT"
   },
   "source": [
    "üî•üî• Wow, that was fast! ‚ö°Ô∏èüî•\n",
    "\n",
    "We now have both a `vocab.json`, which is a list of the most frequent tokens ranked by frequency, and a `merges.txt` list of merges.\n",
    "\n",
    "```json\n",
    "{\n",
    "\t\"<s>\": 0,\n",
    "\t\"<pad>\": 1,\n",
    "\t\"</s>\": 2,\n",
    "\t\"<unk>\": 3,\n",
    "\t\"<mask>\": 4,\n",
    "\t\"!\": 5,\n",
    "\t\"\\\"\": 6,\n",
    "\t\"#\": 7,\n",
    "\t\"$\": 8,\n",
    "\t\"%\": 9,\n",
    "\t\"&\": 10,\n",
    "\t\"'\": 11,\n",
    "\t\"(\": 12,\n",
    "\t\")\": 13,\n",
    "\t# ...\n",
    "}\n",
    "\n",
    "# merges.txt\n",
    "l a\n",
    "ƒ† k\n",
    "o n\n",
    "ƒ† la\n",
    "t a\n",
    "ƒ† e\n",
    "ƒ† d\n",
    "ƒ† p\n",
    "# ...\n",
    "```\n",
    "\n",
    "What is great is that our tokenizer is optimized for Esperanto. Compared to a generic tokenizer trained for English, more native words are represented by a single, unsplit token. Diacritics, i.e. accented characters used in Esperanto ‚Äì `ƒâ`, `ƒù`, `ƒ•`, `ƒµ`, `≈ù`, and `≈≠` ‚Äì are encoded natively. We also represent sequences in a more efficient manner. Here on this corpus, the average length of encoded sequences is ~30% smaller as when using the pretrained GPT-2 tokenizer.\n",
    "\n",
    "Here‚Äôs  how you can use it in `tokenizers`, including handling the RoBERTa special tokens ‚Äì of course, you‚Äôll also be able to use it directly from `transformers`.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "tKVWB8WShT-z",
    "ExecuteTime": {
     "end_time": "2026-02-12T15:16:30.324602Z",
     "start_time": "2026-02-12T15:16:30.047825Z"
    }
   },
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "\n",
    "\n",
    "tokenizer = ByteLevelBPETokenizer(\n",
    "    \"./EsperBERTo/vocab.json\",\n",
    "    \"./EsperBERTo/merges.txt\",\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hO5M3vrAhcuj",
    "ExecuteTime": {
     "end_time": "2026-02-12T15:16:46.772277Z",
     "start_time": "2026-02-12T15:16:46.762210Z"
    }
   },
   "source": [
    "tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "    (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "    (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    ")\n",
    "tokenizer.enable_truncation(max_length=512)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "E3Ye27nchfzq",
    "outputId": "b9812ed2-1ecd-4e1b-d9bd-7de581955e70",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "ExecuteTime": {
     "end_time": "2026-02-12T15:16:53.810300Z",
     "start_time": "2026-02-12T15:16:53.803205Z"
    }
   },
   "source": [
    "tokenizer.encode(\"Mi estas Julien.\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=7, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "X8ya5_7rhjKS",
    "outputId": "e9e08ded-1081-4823-dd81-9d6be1255385",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "ExecuteTime": {
     "end_time": "2026-02-12T15:16:59.836274Z",
     "start_time": "2026-02-12T15:16:59.829774Z"
    }
   },
   "source": [
    "tokenizer.encode(\"Mi estas Julien.\").tokens"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>', 'Mi', 'ƒ†estas', 'ƒ†Juli', 'en', '.', '</s>']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQpUC_CDhnWW"
   },
   "source": [
    "## 3. Train a language model from scratch\n",
    "\n",
    "**Update:** This section follows along the [`run_language_modeling.py`](https://github.com/huggingface/transformers/blob/master/examples/legacy/run_language_modeling.py) script, using our new [`Trainer`](https://github.com/huggingface/transformers/blob/master/src/transformers/trainer.py) directly. Feel free to pick the approach you like best.\n",
    "\n",
    "> We‚Äôll train a RoBERTa-like model, which is a BERT-like with a couple of changes (check the [documentation](https://huggingface.co/transformers/model_doc/roberta.html) for more details).\n",
    "\n",
    "As the model is BERT-like, we‚Äôll train it on a task of *Masked language modeling*, i.e. the predict how to fill arbitrary tokens that we randomly mask in the dataset. This is taken care of by the example script.\n",
    "\n",
    "**Note:** This code below assumes you are using CUDA, but it can also run on other devices like XPUs or TPUs. The framework dynamically detects the available hardware and adjusts accordingly."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kD140sFjh0LQ",
    "outputId": "0bab1f9e-bf7a-4f13-82d3-07fe5866ce78",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "ExecuteTime": {
     "end_time": "2026-02-14T08:11:36.798733004Z",
     "start_time": "2026-02-14T08:11:36.618393169Z"
    }
   },
   "source": [
    "# Check that we have a GPU\n",
    "!nvidia-smi"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Feb 14 09:11:36 2026       \r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 590.48.01              Driver Version: 590.48.01      CUDA Version: 13.1     |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                        |               MIG M. |\r\n",
      "|=========================================+========================+======================|\r\n",
      "|   0  NVIDIA GeForce RTX 3060 Ti     Off |   00000000:01:00.0  On |                  N/A |\r\n",
      "|  0%   23C    P5             39W /  240W |     428MiB /   8192MiB |     32%      Default |\r\n",
      "|                                         |                        |                  N/A |\r\n",
      "+-----------------------------------------+------------------------+----------------------+\r\n",
      "\r\n",
      "+-----------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                              |\r\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\r\n",
      "|        ID   ID                                                               Usage      |\r\n",
      "|=========================================================================================|\r\n",
      "|    0   N/A  N/A            1917      G   /usr/bin/ksecretd                         2MiB |\r\n",
      "|    0   N/A  N/A            2105      G   /usr/bin/kwin_wayland                     6MiB |\r\n",
      "|    0   N/A  N/A            2207      G   /usr/bin/maliit-keyboard                 60MiB |\r\n",
      "|    0   N/A  N/A            2215      G   /usr/bin/Xwayland                        10MiB |\r\n",
      "|    0   N/A  N/A            2274      G   /usr/bin/ksmserver                        2MiB |\r\n",
      "|    0   N/A  N/A            2276      G   /usr/bin/kded6                            2MiB |\r\n",
      "|    0   N/A  N/A            2294      G   /usr/bin/plasmashell                    117MiB |\r\n",
      "|    0   N/A  N/A            2391      G   /usr/bin/kaccess                          2MiB |\r\n",
      "|    0   N/A  N/A            2392      G   ...it-kde-authentication-agent-1          2MiB |\r\n",
      "|    0   N/A  N/A            2400      G   ...ibexec/xdg-desktop-portal-kde          2MiB |\r\n",
      "|    0   N/A  N/A            2485      G   /usr/bin/kdeconnectd                      2MiB |\r\n",
      "|    0   N/A  N/A            2584      G   /usr/bin/xwaylandvideobridge              2MiB |\r\n",
      "|    0   N/A  N/A            2796      G   ...share/Steam/ubuntu12_32/steam          3MiB |\r\n",
      "|    0   N/A  N/A            3063      G   /usr/libexec/baloorunner                  2MiB |\r\n",
      "|    0   N/A  N/A            3092      G   ./steamwebhelper                         22MiB |\r\n",
      "|    0   N/A  N/A            3124    C+G   ...am/ubuntu12_64/steamwebhelper          7MiB |\r\n",
      "|    0   N/A  N/A            4598      G   /usr/bin/kwalletd6                        2MiB |\r\n",
      "+-----------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VNZZs-r6iKAV",
    "outputId": "c8404d6c-7662-4240-c8da-ee89edfaf51b",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "ExecuteTime": {
     "end_time": "2026-02-14T08:11:29.813272132Z",
     "start_time": "2026-02-14T08:11:28.223341728Z"
    }
   },
   "source": [
    "# Check that PyTorch sees it\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0qQzgrBi1OX"
   },
   "source": [
    "### We'll define the following config for the model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LTXXutqeDzPi",
    "ExecuteTime": {
     "end_time": "2026-02-14T08:11:46.439481173Z",
     "start_time": "2026-02-14T08:11:46.415720616Z"
    }
   },
   "source": [
    "from transformers import RobertaConfig\n",
    "\n",
    "config = RobertaConfig(\n",
    "    vocab_size=52_000,\n",
    "    max_position_embeddings=514,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yAwQ82JiE5pi"
   },
   "source": [
    "Now let's re-create our tokenizer in transformers"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4keFBUjQFOD1",
    "ExecuteTime": {
     "end_time": "2026-02-12T15:31:30.160689Z",
     "start_time": "2026-02-12T15:31:30.102336Z"
    }
   },
   "source": [
    "#from transformers import RobertaTokenizerFast\n",
    "#tokenizer = RobertaTokenizerFast.from_pretrained(\"./EsperBERTo\", max_len=512)"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Nota:\n",
    "- Cambie de RobertaTokenizerFast a RobertaTokenizer, porque Fast rechazaba la estructura del fichero merges.txt, aparentemente solo acepta 2 \"palabras\" por linea\n",
    "- En este sentido RobertaTokenizer es mas flexible\n",
    "- Una vez procesado almacene el modelo\n",
    "- Volvi a generar a RobertaTokenizerPast a partir del modelo \"lento\""
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T08:11:50.340034170Z",
     "start_time": "2026-02-14T08:11:49.777360285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import RobertaTokenizer, RobertaTokenizerFast\n",
    "\n",
    "slow_tokenizer = RobertaTokenizer.from_pretrained(\"./EsperBERTo\", max_len=512)\n",
    "slow_tokenizer.save_pretrained(\"./EsperBERTo\") #Save the model\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"./EsperBERTo\", max_len=512)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6yNCw-3hFv9h"
   },
   "source": [
    "Finally let's initialize our model.\n",
    "\n",
    "**Important:**\n",
    "\n",
    "As we are training from scratch, we only initialize from a config, not from an existing pretrained model or checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BzMqR-dzF4Ro",
    "ExecuteTime": {
     "end_time": "2026-02-14T08:11:53.884588996Z",
     "start_time": "2026-02-14T08:11:52.728473010Z"
    }
   },
   "source": [
    "from transformers import RobertaForMaskedLM\n",
    "\n",
    "model = RobertaForMaskedLM(config=config)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jU6JhBSTKiaM",
    "outputId": "35879a60-2915-4894-f702-2d649cfa398a",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "ExecuteTime": {
     "end_time": "2026-02-14T08:11:55.252444243Z",
     "start_time": "2026-02-14T08:11:55.218365209Z"
    }
   },
   "source": [
    "model.num_parameters()\n",
    "# => 84 million parameters"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83504416"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBtUHRMliOLM"
   },
   "source": [
    "### Now let's build our training Dataset\n",
    "\n",
    "We'll build our dataset by applying our tokenizer to our text file.\n",
    "\n",
    "Here, as we only have one text file, we don't even need to customize our `Dataset`. We'll just use the `LineByLineDataset` out-of-the-box."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Nota: **LineByLineTextDataset fue deprecado**\n",
    "\n",
    "Por lo que usaremos su equivalente moderno: load_dataset"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\"\"\"\n",
    "%%time\n",
    "from transformers import LineByLineTextDataset\n",
    "\n",
    "dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"oscar.eo.txt\",\n",
    "    block_size=128,\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-14T08:11:59.997255469Z",
     "start_time": "2026-02-14T08:11:58.204235949Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"text\", data_files={\"train\": \"data/oscar.eo.txt\"})\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation =True, max_length=128)\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function,\n",
    "                                      batched=True,\n",
    "                                      num_proc=4,\n",
    "                                      remove_columns=[\"text\"])\n",
    "dataset = tokenized_datasets[\"train\"]"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDLs73HcIHk5"
   },
   "source": [
    "Like in the [`run_language_modeling.py`](https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_language_modeling.py) script, we need to define a data_collator.\n",
    "\n",
    "This is just a small helper that will help us batch different samples of the dataset together into an object that PyTorch knows how to perform backprop on."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zTgWPa9Dipk2",
    "ExecuteTime": {
     "end_time": "2026-02-14T08:12:03.156457907Z",
     "start_time": "2026-02-14T08:12:03.132503884Z"
    }
   },
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ri2BIQKqjfHm"
   },
   "source": [
    "### Finally, we are all set to initialize our Trainer"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T21:38:50.237930721Z",
     "start_time": "2026-02-13T21:38:50.186111958Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.1.0\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-13T20:36:54.124369326Z",
     "start_time": "2026-02-13T20:36:53.371515853Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install transformers[torch]",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (5.1.0)\r\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from transformers[torch]) (1.4.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from transformers[torch]) (2.2.6)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from transformers[torch]) (26.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from transformers[torch]) (6.0.3)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from transformers[torch]) (2026.1.15)\r\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from transformers[torch]) (0.22.2)\r\n",
      "Requirement already satisfied: typer-slim in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from transformers[torch]) (0.23.1)\r\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from transformers[torch]) (0.7.0)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from transformers[torch]) (4.67.3)\r\n",
      "Requirement already satisfied: torch>=2.4 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from transformers[torch]) (2.10.0)\r\n",
      "Requirement already satisfied: accelerate>=1.1.0 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from transformers[torch]) (1.12.0)\r\n",
      "Requirement already satisfied: psutil in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from accelerate>=1.1.0->transformers[torch]) (7.2.2)\r\n",
      "Requirement already satisfied: filelock in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers[torch]) (3.21.2)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers[torch]) (2025.10.0)\r\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers[torch]) (1.2.0)\r\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers[torch]) (0.28.1)\r\n",
      "Requirement already satisfied: shellingham in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers[torch]) (1.5.4)\r\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from huggingface-hub<2.0,>=1.3.0->transformers[torch]) (4.15.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from torch>=2.4->transformers[torch]) (1.14.0)\r\n",
      "Requirement already satisfied: networkx>=2.5.1 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from torch>=2.4->transformers[torch]) (3.4.2)\r\n",
      "Requirement already satisfied: jinja2 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from torch>=2.4->transformers[torch]) (3.1.6)\r\n",
      "Requirement already satisfied: cuda-bindings==12.9.4 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from torch>=2.4->transformers[torch]) (12.9.4)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from torch>=2.4->transformers[torch]) (12.8.93)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from torch>=2.4->transformers[torch]) (12.8.90)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from torch>=2.4->transformers[torch]) (12.8.90)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from torch>=2.4->transformers[torch]) (9.10.2.21)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from torch>=2.4->transformers[torch]) (12.8.4.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from torch>=2.4->transformers[torch]) (11.3.3.83)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from torch>=2.4->transformers[torch]) (10.3.9.90)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from torch>=2.4->transformers[torch]) (11.7.3.90)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from torch>=2.4->transformers[torch]) (12.5.8.93)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from torch>=2.4->transformers[torch]) (0.7.1)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from torch>=2.4->transformers[torch]) (2.27.5)\r\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from torch>=2.4->transformers[torch]) (3.4.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from torch>=2.4->transformers[torch]) (12.8.90)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from torch>=2.4->transformers[torch]) (12.8.93)\r\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from torch>=2.4->transformers[torch]) (1.13.1.3)\r\n",
      "Requirement already satisfied: triton==3.6.0 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from torch>=2.4->transformers[torch]) (3.6.0)\r\n",
      "Requirement already satisfied: cuda-pathfinder~=1.1 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from cuda-bindings==12.9.4->torch>=2.4->transformers[torch]) (1.3.4)\r\n",
      "Requirement already satisfied: typer>=0.23.1 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from typer-slim->transformers[torch]) (0.23.1)\r\n",
      "Requirement already satisfied: anyio in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers[torch]) (4.12.1)\r\n",
      "Requirement already satisfied: certifi in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers[torch]) (2026.1.4)\r\n",
      "Requirement already satisfied: httpcore==1.* in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers[torch]) (1.0.9)\r\n",
      "Requirement already satisfied: idna in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers[torch]) (3.11)\r\n",
      "Requirement already satisfied: h11>=0.16 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers[torch]) (0.16.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=2.4->transformers[torch]) (1.3.0)\r\n",
      "Requirement already satisfied: click>=8.0.0 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from typer>=0.23.1->typer-slim->transformers[torch]) (8.3.1)\r\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from typer>=0.23.1->typer-slim->transformers[torch]) (14.3.2)\r\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from typer>=0.23.1->typer-slim->transformers[torch]) (0.0.4)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from jinja2->torch>=2.4->transformers[torch]) (3.0.3)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.23.1->typer-slim->transformers[torch]) (4.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.23.1->typer-slim->transformers[torch]) (2.19.2)\r\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers[torch]) (1.3.1)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/roroal23/proyectos/oeg/.venv/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.23.1->typer-slim->transformers[torch]) (0.1.2)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m24.3.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m26.0.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YpvnFFmZJD-N",
    "ExecuteTime": {
     "end_time": "2026-02-14T08:12:11.177644083Z",
     "start_time": "2026-02-14T08:12:10.934954195Z"
    }
   },
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./EsperBERTo\",\n",
    "    #overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    gradient_accumulation_steps=16,\n",
    "    #gradient_accumulation_steps=8,\n",
    "    per_device_train_batch_size=16,\n",
    "    #per_gpu_train_batch_size=64,\n",
    "    save_steps=5000,\n",
    "    save_total_limit=2,\n",
    "    logging_steps=500,\n",
    "    prediction_loss_only=True,\n",
    "    #dataloader_num_workers=2,\n",
    "    dataloader_num_workers=1,\n",
    "\n",
    "    fp16=True,\n",
    "\n",
    "    #Configuracion para torre con RTX 3060, entrenamiento reducido a alrededor de 1 hora con <5Gib de uso\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6sASa36Nf-N"
   },
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VmaHZXzmkNtJ",
    "outputId": "a19880cb-bcc6-4885-bf24-c2c6d0f56d1e",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 738,
     "referenced_widgets": [
      "a58a66392b644b1384661e850c077a6c",
      "a491e8caa0a048beb3b5259f14eb233f",
      "837c9ddc3d594e088891874560c646b8",
      "dbf50873d62c4ba39321faefbed0cca5",
      "40bf955ba0284e84b198da6be8654219",
      "fe20a8dae6e84628b5076d02183090f5",
      "93b3f9eae3cb4e3e859cf456e3547c6d",
      "6feb10aeb43147e6aba028d065947ae8",
      "0989d41a4da24e9ebff377e02127642c",
      "42c6061ef7e44f179db5a6e3551c0f17",
      "d295dd80550447d88da0f04ce36a22ff",
      "04e7e6d291da49d5816dc98a2904e95c",
      "e7d8c3a4fecd40778e32966b29ea65a1",
      "016d7c8318f742c1943464b08232a510",
      "8388e9da9da4492c98c19235ca5fc1b5",
      "39c23c6a972b419eb2eeeebafeaedc22"
     ]
    },
    "ExecuteTime": {
     "end_time": "2026-02-14T09:27:37.210646321Z",
     "start_time": "2026-02-14T08:12:14.007154992Z"
    }
   },
   "source": [
    "%%time\n",
    "trainer.train()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3807' max='3807' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3807/3807 1:15:21, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>52.918910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>43.458223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>42.784863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>42.004195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>40.771758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>38.537070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>36.771543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "c02e45aca3dbf65876495448b1984baa"
     }
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 8min 39s, sys: 16.3 s, total: 1h 8min 55s\n",
      "Wall time: 1h 15min 23s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3807, training_loss=41.94483178601917, metrics={'train_runtime': 4523.0526, 'train_samples_per_second': 215.424, 'train_steps_per_second': 0.842, 'total_flos': 3.2307042064367616e+16, 'train_loss': 41.94483178601917, 'epoch': 1.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZkooHz1-_2h"
   },
   "source": [
    "#### üéâ Save final model (+ tokenizer + config) to disk"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QDNgPls7_l13",
    "ExecuteTime": {
     "end_time": "2026-02-14T09:31:00.788702424Z",
     "start_time": "2026-02-14T09:31:00.224487495Z"
    }
   },
   "source": [
    "trainer.save_model(\"./EsperBERTo\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  2.42it/s]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0caceCy_p1-"
   },
   "source": [
    "## 4. Check that the LM actually trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIQJ8ND_AEhl"
   },
   "source": [
    "Aside from looking at the training and eval losses going down, the easiest way to check whether our language model is learning anything interesting is via the `FillMaskPipeline`.\n",
    "\n",
    "Pipelines are simple wrappers around tokenizers and models, and the 'fill-mask' one will let you input a sequence containing a masked token (here, `<mask>`) and return a list of the most probable filled sequences, with their probabilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ltXgXyCbAJLY",
    "ExecuteTime": {
     "end_time": "2026-02-14T09:31:17.274409187Z",
     "start_time": "2026-02-14T09:31:16.342392689Z"
    }
   },
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"./EsperBERTo\",\n",
    "    tokenizer=\"./EsperBERTo\"\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 106/106 [00:00<00:00, 1262.62it/s, Materializing param=roberta.encoder.layer.5.output.dense.weight]             \n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UIvgZ3S6AO0z",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "outputId": "5f3d2f00-abdc-44a9-9c1b-75e3ec328576",
    "ExecuteTime": {
     "end_time": "2026-02-14T09:31:20.922037013Z",
     "start_time": "2026-02-14T09:31:20.793704200Z"
    }
   },
   "source": [
    "# The sun <mask>.\n",
    "# =>\n",
    "\n",
    "fill_mask(\"La suno <mask>.\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.35176873207092285,\n",
       "  'token': 18,\n",
       "  'token_str': '.',\n",
       "  'sequence': 'La suno ..'},\n",
       " {'score': 0.10754923522472382,\n",
       "  'token': 83,\n",
       "  'token_str': 'o',\n",
       "  'sequence': 'La suno o.'},\n",
       " {'score': 0.04824163392186165,\n",
       "  'token': 69,\n",
       "  'token_str': 'a',\n",
       "  'sequence': 'La suno a.'},\n",
       " {'score': 0.03688143193721771,\n",
       "  'token': 77,\n",
       "  'token_str': 'i',\n",
       "  'sequence': 'La suno i.'},\n",
       " {'score': 0.029939131811261177,\n",
       "  'token': 73,\n",
       "  'token_str': 'e',\n",
       "  'sequence': 'La suno e.'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0qCyyhNAWZi"
   },
   "source": [
    "Ok, simple syntax/grammar works. Let‚Äôs try a slightly more interesting prompt:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YZ9HSQxAAbme",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "outputId": "aabfeedc-b1d0-4837-b01d-cd42726a5a3d",
    "ExecuteTime": {
     "end_time": "2026-02-14T09:31:42.619450828Z",
     "start_time": "2026-02-14T09:31:42.551910272Z"
    }
   },
   "source": [
    "fill_mask(\"Jen la komenco de bela <mask>.\")\n",
    "\n",
    "# This is the beginning of a beautiful <mask>.\n",
    "# =>"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.16486455500125885,\n",
       "  'token': 83,\n",
       "  'token_str': 'o',\n",
       "  'sequence': 'Jen la komenco de bela o.'},\n",
       " {'score': 0.12627507746219635,\n",
       "  'token': 18,\n",
       "  'token_str': '.',\n",
       "  'sequence': 'Jen la komenco de bela ..'},\n",
       " {'score': 0.04670954868197441,\n",
       "  'token': 82,\n",
       "  'token_str': 'n',\n",
       "  'sequence': 'Jen la komenco de bela n.'},\n",
       " {'score': 0.04132508113980293,\n",
       "  'token': 69,\n",
       "  'token_str': 'a',\n",
       "  'sequence': 'Jen la komenco de bela a.'},\n",
       " {'score': 0.04064961150288582,\n",
       "  'token': 87,\n",
       "  'token_str': 's',\n",
       "  'sequence': 'Jen la komenco de bela s.'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Nota: **Resultados del experimento\"**\n",
    "\n",
    "El experimento fall√≥, el modelo no fue capaz de completar las oraciones correctamente. Pienso que ocurri√≥ por las siguientes razones:\n",
    "- El art√≠culo de JC explicaba que utiliz√≥ cerca de 3GB de datos para su entrenamiento, sin embargo, el fichero oscar.eo.txt que usamos para entrenar apenas tiene 300MB. Con ese volumen de datos y la gran cantidad de tokens que hay, el modelo apenas pudo encontrar relaciones significativas entre palabras.\n",
    "- El dataset completo que debia utilizar es la concatenaci√≥n de oscar.eo.txt junto a multiples entradas del Leipzig Corpora Collection. Estas entradas no tienen un formato \"limpio\" (muchas de ellas tienen el n√∫mero de linea o caracteres como <<), por lo que asumo que en el art√≠culo no se incluy√≥ una etapa de preprocesado previa. La aplicare en la siguiente prueba\n",
    "- El n√∫mero de epocas es insuficiente: Una sola epoca no permite que el modelo aprenda sobre las palabras poco frecuentes. Se recomienda como m√≠nimo 5 epocas para generar un resultado aceptable, y hasta 40 epocas para refinarlo.\n"
   ]
  }
 ]
}
