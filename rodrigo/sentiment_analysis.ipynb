{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Fine-Tuning BERT for Sentiment Analysis",
   "id": "d733fd402d68f34a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Siguiendo los pasos que aparecen en \"A Complete Guide to BERT with Code\"",
   "id": "324229f400d5006e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T16:46:37.126520Z",
     "start_time": "2026-02-06T16:46:37.034365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ],
   "id": "cb8be2ba4135b5b0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "275dcccf15184729b71e50f7fd02782f"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-06T15:32:08.543064Z",
     "start_time": "2026-02-06T15:32:08.369127Z"
    }
   },
   "source": "import pandas as pd",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T15:44:49.522242Z",
     "start_time": "2026-02-06T15:44:49.177009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('IMDB Dataset.csv')\n",
    "df.head(5)"
   ],
   "id": "c008800aedc23bf3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1 Load and Preprocess the Dataset",
   "id": "7ca3077ff828648a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cleaning the reviews",
   "id": "a36bab22f5f482a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T15:44:52.991695Z",
     "start_time": "2026-02-06T15:44:51.112127Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(f\"Original:\\n{df.iloc[1]['review'][0:72]}\")\n",
    "\n",
    "#Remove HTML break tags <br />\n",
    "df['review_cleaned'] = df['review'].apply(lambda x: x.replace('<br />', ''))\n",
    "print(f\"With no break tags:\\n{df.iloc[1]['review_cleaned'][0:72]}\")\n",
    "\n",
    "#Remove unnecesary whitespace\n",
    "df['review_cleaned'] = df['review_cleaned'].replace(r'\\s+', ' ', regex=True)\n",
    "print(f\"Cleaned:\\n{df.iloc[1]['review_cleaned'][0:72]}\")"
   ],
   "id": "e7eba8d4c59c6df0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "A wonderful little production. <br /><br />The filming technique is very\n",
      "With no break tags:\n",
      "A wonderful little production. The filming technique is very unassuming-\n",
      "Cleaned:\n",
      "A wonderful little production. The filming technique is very unassuming-\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Encoding the sentiment",
   "id": "a8a988ad830eebe1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T15:44:54.754493Z",
     "start_time": "2026-02-06T15:44:54.736313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df['sentiment_encoded'] = df['sentiment'].apply(lambda x:0 if x == 'negative' else 1)\n",
    "df.head(5)"
   ],
   "id": "7dcbc84d4dbdb196",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                              review sentiment  \\\n",
       "0  One of the other reviewers has mentioned that ...  positive   \n",
       "1  A wonderful little production. <br /><br />The...  positive   \n",
       "2  I thought this was a wonderful way to spend ti...  positive   \n",
       "3  Basically there's a family where a little boy ...  negative   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive   \n",
       "\n",
       "                                      review_cleaned  sentiment_encoded  \n",
       "0  One of the other reviewers has mentioned that ...                  1  \n",
       "1  A wonderful little production. The filming tec...                  1  \n",
       "2  I thought this was a wonderful way to spend ti...                  1  \n",
       "3  Basically there's a family where a little boy ...                  0  \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...                  1  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review_cleaned</th>\n",
       "      <th>sentiment_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "      <td>A wonderful little production. The filming tec...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2 Tokenize the Data",
   "id": "9da3e335f8c0a1b4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T15:47:59.179401Z",
     "start_time": "2026-02-06T15:47:59.176696Z"
    }
   },
   "cell_type": "code",
   "source": "from transformers import BertTokenizer",
   "id": "390c87605bf06e3f",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T15:48:39.870397Z",
     "start_time": "2026-02-06T15:48:37.390887Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenizer"
   ],
   "id": "13938305cace1cc4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizer(name_or_path='bert-base-uncased', vocab_size=30522, model_max_length=512, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Visualization for a single review",
   "id": "4008f59c29cdaeed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T16:00:29.833219Z",
     "start_time": "2026-02-06T16:00:29.830237Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#Encode a sample sentence\n",
    "sample = \"I liked this movie\"\n",
    "#We can return token_ids tensor in pytorch, numpy or tensor flow format\n",
    "token_ids = tokenizer.encode(sample, return_tensors='np')[0]\n",
    "print(f\"Token IDs:\\n{token_ids}\")\n",
    "\n",
    "#Convert token_ids back to tokens. Just to visualize special tokens added\n",
    "tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "print(f\"Tokens:\\n{tokens}\")"
   ],
   "id": "23656dc9c4d95e8a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      "[ 101 1045 4669 2023 3185  102]\n",
      "Tokens:\n",
      "['[CLS]', 'i', 'liked', 'this', 'movie', '[SEP]']\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T16:08:44.999154Z",
     "start_time": "2026-02-06T16:08:44.992806Z"
    }
   },
   "cell_type": "code",
   "source": [
    "review = df['review_cleaned'].iloc[0]\n",
    "token_ids = tokenizer.encode(review, max_length = 512, padding = 'max_length', truncation = True, return_tensors = 'pt')\n",
    "token_ids #Padding starts at 393th token"
   ],
   "id": "44eb98cae2654d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2028,  1997,  1996,  2060, 15814,  2038,  3855,  2008,  2044,\n",
       "          3666,  2074,  1015, 11472,  2792,  2017,  1005,  2222,  2022, 13322,\n",
       "          1012,  2027,  2024,  2157,  1010,  2004,  2023,  2003,  3599,  2054,\n",
       "          3047,  2007,  2033,  1012,  1996,  2034,  2518,  2008,  4930,  2033,\n",
       "          2055, 11472,  2001,  2049, 24083,  1998,  4895, 10258,  2378,  8450,\n",
       "          5019,  1997,  4808,  1010,  2029,  2275,  1999,  2157,  2013,  1996,\n",
       "          2773,  2175,  1012,  3404,  2033,  1010,  2023,  2003,  2025,  1037,\n",
       "          2265,  2005,  1996,  8143, 18627,  2030,  5199,  3593,  1012,  2023,\n",
       "          2265,  8005,  2053, 17957,  2007, 12362,  2000,  5850,  1010,  3348,\n",
       "          2030,  4808,  1012,  2049,  2003, 13076,  1010,  1999,  1996,  4438,\n",
       "          2224,  1997,  1996,  2773,  1012,  2009,  2003,  2170, 11472,  2004,\n",
       "          2008,  2003,  1996,  8367,  2445,  2000,  1996, 17411,  4555,  3036,\n",
       "          2110,  7279,  4221, 12380,  2854,  1012,  2009,  7679,  3701,  2006,\n",
       "         14110,  2103,  1010,  2019,  6388,  2930,  1997,  1996,  3827,  2073,\n",
       "          2035,  1996,  4442,  2031,  3221, 21430,  1998,  2227, 20546,  2015,\n",
       "          1010,  2061,  9394,  2003,  2025,  2152,  2006,  1996, 11376,  1012,\n",
       "          7861,  2103,  2003,  2188,  2000,  2116,  1012,  1012, 26030,  2015,\n",
       "          1010,  7486,  1010, 18542, 10230,  1010,  7402,  2015,  1010,  8135,\n",
       "          1010, 16773,  1010,  3493,  1998,  2062,  1012,  1012,  1012,  1012,\n",
       "          2061,  8040, 16093, 28331,  1010,  2331, 14020,  1010, 26489,  6292,\n",
       "         24069,  1998, 22824, 10540,  2024,  2196,  2521,  2185,  1012,  1045,\n",
       "          2052,  2360,  1996,  2364,  5574,  1997,  1996,  2265,  2003,  2349,\n",
       "          2000,  1996,  2755,  2008,  2009,  3632,  2073,  2060,  3065,  2876,\n",
       "          1005,  1056,  8108,  1012,  5293,  3492,  4620,  4993,  2005,  7731,\n",
       "          9501,  1010,  5293, 11084,  1010,  5293,  7472,  1012,  1012,  1012,\n",
       "         11472,  2987,  1005,  1056,  6752,  2105,  1012,  1996,  2034,  2792,\n",
       "          1045,  2412,  2387,  4930,  2033,  2004,  2061, 11808,  2009,  2001,\n",
       "         16524,  1010,  1045,  2481,  1005,  1056,  2360,  1045,  2001,  3201,\n",
       "          2005,  2009,  1010,  2021,  2004,  1045,  3427,  2062,  1010,  1045,\n",
       "          2764,  1037,  5510,  2005, 11472,  1010,  1998,  2288, 17730,  2000,\n",
       "          1996,  2152,  3798,  1997,  8425,  4808,  1012,  2025,  2074,  4808,\n",
       "          1010,  2021, 21321,  1006, 15274,  4932,  2040,  1005,  2222,  2022,\n",
       "          2853,  2041,  2005,  1037, 15519,  1010, 13187,  2040,  1005,  2222,\n",
       "          3102,  2006,  2344,  1998,  2131,  2185,  2007,  2009,  1010,  2092,\n",
       "          5450,  2098,  1010,  2690,  2465, 13187,  2108,  2357,  2046,  3827,\n",
       "          7743,  2229,  2349,  2000,  2037,  3768,  1997,  2395,  4813,  2030,\n",
       "          3827,  3325,  1007,  3666, 11472,  1010,  2017,  2089,  2468,  6625,\n",
       "          2007,  2054,  2003,  8796, 10523,  1012,  1012,  1012,  1012,  2008,\n",
       "          2015,  2065,  2017,  2064,  2131,  1999,  3543,  2007,  2115,  9904,\n",
       "          2217,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Nota: tokenizer.encode() puede devolver los token_ids tanto como un tensor de numpy, pytorch or tensorflow. Para este tipo de proyectos se recomienda el formato pytorch ya que esta mejor preparado para usarse en CUDA",
   "id": "b0d59b6bb98516d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T16:17:33.649102Z",
     "start_time": "2026-02-06T16:17:33.619009Z"
    }
   },
   "cell_type": "code",
   "source": [
    "review = df['review_cleaned'].iloc[0]\n",
    "batch_encoder = tokenizer.encode_plus(review, max_length = 512, padding = 'max_length', truncation = True, return_tensors = 'pt')"
   ],
   "id": "2c558d0b928c7fde",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "BertTokenizer has no attribute encode_plus",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[33]\u001B[39m\u001B[32m, line 2\u001B[39m\n\u001B[32m      1\u001B[39m review = df[\u001B[33m'\u001B[39m\u001B[33mreview_cleaned\u001B[39m\u001B[33m'\u001B[39m].iloc[\u001B[32m0\u001B[39m]\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m batch_encoder = \u001B[43mtokenizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mencode_plus\u001B[49m(review, max_length = \u001B[32m512\u001B[39m, padding = \u001B[33m'\u001B[39m\u001B[33mmax_length\u001B[39m\u001B[33m'\u001B[39m, truncation = \u001B[38;5;28;01mTrue\u001B[39;00m, return_tensors = \u001B[33m'\u001B[39m\u001B[33mpt\u001B[39m\u001B[33m'\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/proyectos/oeg/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1291\u001B[39m, in \u001B[36mPreTrainedTokenizerBase.__getattr__\u001B[39m\u001B[34m(self, key)\u001B[39m\n\u001B[32m   1288\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.convert_tokens_to_ids(tokens) \u001B[38;5;28;01mif\u001B[39;00m key != key_without_id \u001B[38;5;28;01melse\u001B[39;00m tokens\n\u001B[32m   1290\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m key \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.\u001B[34m__dict__\u001B[39m:\n\u001B[32m-> \u001B[39m\u001B[32m1291\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.\u001B[34m__class__\u001B[39m.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m has no attribute \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mkey\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m\"\u001B[39m)\n\u001B[32m   1292\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m().\u001B[34m__getattr__\u001B[39m(key)\n",
      "\u001B[31mAttributeError\u001B[39m: BertTokenizer has no attribute encode_plus"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "**Nota: En la versión actual de la libreria transformers, ya no se usa encode_plus(). Lo que solia hacer encode_plus() ahora se maneja automaticamente desde la funcion tokenizer()**",
   "id": "cbc9e74b5ea56437"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T16:21:10.150525Z",
     "start_time": "2026-02-06T16:21:10.145726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "review = df['review_cleaned'].iloc[0]\n",
    "batch_encoder = tokenizer(review, max_length = 512, padding = 'max_length', truncation = True, return_tensors = 'pt')"
   ],
   "id": "37967b1f6e9afeab",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T16:21:11.756344Z",
     "start_time": "2026-02-06T16:21:11.748227Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Batch encoder keys:\\n{batch_encoder.keys()}\\n\")",
   "id": "7290236a54689657",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch encoder keys:\n",
      "KeysView({'input_ids': tensor([[  101,  2028,  1997,  1996,  2060, 15814,  2038,  3855,  2008,  2044,\n",
      "          3666,  2074,  1015, 11472,  2792,  2017,  1005,  2222,  2022, 13322,\n",
      "          1012,  2027,  2024,  2157,  1010,  2004,  2023,  2003,  3599,  2054,\n",
      "          3047,  2007,  2033,  1012,  1996,  2034,  2518,  2008,  4930,  2033,\n",
      "          2055, 11472,  2001,  2049, 24083,  1998,  4895, 10258,  2378,  8450,\n",
      "          5019,  1997,  4808,  1010,  2029,  2275,  1999,  2157,  2013,  1996,\n",
      "          2773,  2175,  1012,  3404,  2033,  1010,  2023,  2003,  2025,  1037,\n",
      "          2265,  2005,  1996,  8143, 18627,  2030,  5199,  3593,  1012,  2023,\n",
      "          2265,  8005,  2053, 17957,  2007, 12362,  2000,  5850,  1010,  3348,\n",
      "          2030,  4808,  1012,  2049,  2003, 13076,  1010,  1999,  1996,  4438,\n",
      "          2224,  1997,  1996,  2773,  1012,  2009,  2003,  2170, 11472,  2004,\n",
      "          2008,  2003,  1996,  8367,  2445,  2000,  1996, 17411,  4555,  3036,\n",
      "          2110,  7279,  4221, 12380,  2854,  1012,  2009,  7679,  3701,  2006,\n",
      "         14110,  2103,  1010,  2019,  6388,  2930,  1997,  1996,  3827,  2073,\n",
      "          2035,  1996,  4442,  2031,  3221, 21430,  1998,  2227, 20546,  2015,\n",
      "          1010,  2061,  9394,  2003,  2025,  2152,  2006,  1996, 11376,  1012,\n",
      "          7861,  2103,  2003,  2188,  2000,  2116,  1012,  1012, 26030,  2015,\n",
      "          1010,  7486,  1010, 18542, 10230,  1010,  7402,  2015,  1010,  8135,\n",
      "          1010, 16773,  1010,  3493,  1998,  2062,  1012,  1012,  1012,  1012,\n",
      "          2061,  8040, 16093, 28331,  1010,  2331, 14020,  1010, 26489,  6292,\n",
      "         24069,  1998, 22824, 10540,  2024,  2196,  2521,  2185,  1012,  1045,\n",
      "          2052,  2360,  1996,  2364,  5574,  1997,  1996,  2265,  2003,  2349,\n",
      "          2000,  1996,  2755,  2008,  2009,  3632,  2073,  2060,  3065,  2876,\n",
      "          1005,  1056,  8108,  1012,  5293,  3492,  4620,  4993,  2005,  7731,\n",
      "          9501,  1010,  5293, 11084,  1010,  5293,  7472,  1012,  1012,  1012,\n",
      "         11472,  2987,  1005,  1056,  6752,  2105,  1012,  1996,  2034,  2792,\n",
      "          1045,  2412,  2387,  4930,  2033,  2004,  2061, 11808,  2009,  2001,\n",
      "         16524,  1010,  1045,  2481,  1005,  1056,  2360,  1045,  2001,  3201,\n",
      "          2005,  2009,  1010,  2021,  2004,  1045,  3427,  2062,  1010,  1045,\n",
      "          2764,  1037,  5510,  2005, 11472,  1010,  1998,  2288, 17730,  2000,\n",
      "          1996,  2152,  3798,  1997,  8425,  4808,  1012,  2025,  2074,  4808,\n",
      "          1010,  2021, 21321,  1006, 15274,  4932,  2040,  1005,  2222,  2022,\n",
      "          2853,  2041,  2005,  1037, 15519,  1010, 13187,  2040,  1005,  2222,\n",
      "          3102,  2006,  2344,  1998,  2131,  2185,  2007,  2009,  1010,  2092,\n",
      "          5450,  2098,  1010,  2690,  2465, 13187,  2108,  2357,  2046,  3827,\n",
      "          7743,  2229,  2349,  2000,  2037,  3768,  1997,  2395,  4813,  2030,\n",
      "          3827,  3325,  1007,  3666, 11472,  1010,  2017,  2089,  2468,  6625,\n",
      "          2007,  2054,  2003,  8796, 10523,  1012,  1012,  1012,  1012,  2008,\n",
      "          2015,  2065,  2017,  2064,  2131,  1999,  3543,  2007,  2115,  9904,\n",
      "          2217,  1012,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])})\n",
      "\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T16:21:17.268063Z",
     "start_time": "2026-02-06T16:21:17.263881Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Attention mask:\\n{batch_encoder['attention_mask']}\")",
   "id": "21a6bc84f2bb172f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention mask:\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Encoding all reviews",
   "id": "598f6a0ca5606f44"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T16:24:31.955663Z",
     "start_time": "2026-02-06T16:24:31.952609Z"
    }
   },
   "cell_type": "code",
   "source": "import torch",
   "id": "e671ff9de6c31ddd",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T16:27:58.201266Z",
     "start_time": "2026-02-06T16:27:12.933080Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%%time\n",
    "\n",
    "token_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "#Encode each review\n",
    "for review in df['review_cleaned']:\n",
    "    batch_encoder = tokenizer(review, max_length = 512, padding = 'max_length', truncation = True, return_tensors = 'pt')\n",
    "    token_ids.append(batch_encoder['input_ids'])\n",
    "    attention_masks.append(batch_encoder['attention_mask'])\n",
    "\n",
    "#Convert token_ids and attention_masks to pytorch tensors\n",
    "token_ids = torch.cat(token_ids, dim = 0)\n",
    "attention_masks = torch.cat(attention_masks, dim = 0)"
   ],
   "id": "13bc89a266fa67ed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 45.1 s, sys: 178 ms, total: 45.3 s\n",
      "Wall time: 45.3 s\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3 Create the Train and Validation DataLoaders",
   "id": "637eebc984a81076"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T16:32:02.098635Z",
     "start_time": "2026-02-06T16:32:01.119989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ],
   "id": "e1dcc4bca726d536",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T16:40:33.001100Z",
     "start_time": "2026-02-06T16:40:32.836473Z"
    }
   },
   "cell_type": "code",
   "source": [
    "val_size = 0.1\n",
    "\n",
    "#Split the token IDs\n",
    "train_ids, val_ids = train_test_split(token_ids, test_size=val_size, shuffle=False)\n",
    "\n",
    "#Split the attention masks\n",
    "train_masks, val_masks = train_test_split(attention_masks, test_size=val_size, shuffle=False)\n",
    "\n",
    "#Split the labels\n",
    "labels = torch.tensor(df['sentiment_encoded'].values)\n",
    "train_labels, val_labels = train_test_split(labels, test_size=val_size, shuffle=False)\n",
    "\n",
    "#Create the DataLoaders\n",
    "train_data = TensorDataset(train_ids, train_masks, train_labels)\n",
    "train_dataloader = DataLoader(train_data, shuffle = True, batch_size = 16)\n",
    "val_data = TensorDataset(val_ids, val_masks, val_labels)\n",
    "val_dataloader = DataLoader(val_data, batch_size = 16)"
   ],
   "id": "3367f0d3857ddbef",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Nota: **Sobre suffle**\n",
    "   - Al hacer las particiones es importante no mezclar los elementos (shuffle=False). De otro modo, perderiamos la asociaciones entre ids, mascaras y labels\n",
    "   - En train_dataloader se tiene suffle=True porque el objetivo del conjunto de entrenamiento es que el modelo aprenda los patrones del texto, e ignore información asociada a las posiciones de los elementos\n",
    "   - En data_loader hacer suffle es innecesario porque el modelo no obtiene información a partir de el, solo se usa para evaluar su rendimiento"
   ],
   "id": "eb971d6fb910789f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4 Instantiate a BERT model",
   "id": "cf3ae23e8d894963"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T16:43:32.926936Z",
     "start_time": "2026-02-06T16:43:32.209477Z"
    }
   },
   "cell_type": "code",
   "source": "from transformers import BertForSequenceClassification",
   "id": "beb9ac962296b83d",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T16:53:48.676471Z",
     "start_time": "2026-02-06T16:53:47.996790Z"
    }
   },
   "cell_type": "code",
   "source": "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 2)",
   "id": "81d332a388285c63",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 631.30it/s, Materializing param=bert.pooler.dense.weight]                               \n",
      "\u001B[1mBertForSequenceClassification LOAD REPORT\u001B[0m from: bert-base-uncased\n",
      "Key                                        | Status     | \n",
      "-------------------------------------------+------------+-\n",
      "cls.seq_relationship.bias                  | UNEXPECTED | \n",
      "cls.seq_relationship.weight                | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.bias   | UNEXPECTED | \n",
      "cls.predictions.bias                       | UNEXPECTED | \n",
      "cls.predictions.transform.LayerNorm.weight | UNEXPECTED | \n",
      "cls.predictions.transform.dense.weight     | UNEXPECTED | \n",
      "cls.predictions.transform.dense.bias       | UNEXPECTED | \n",
      "classifier.bias                            | MISSING    | \n",
      "classifier.weight                          | MISSING    | \n",
      "\n",
      "\u001B[3mNotes:\n",
      "- UNEXPECTED\u001B[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\u001B[3m\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\u001B[0m\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Nota: Aunque parezca un error, este status report es normal. Significa que el modelo aun no ha sido entrenado para nuestra tarea, por lo que los pesos de la capa lineal aun no tienen valor definido",
   "id": "46a8efbc019ef2e8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5 Instantitate an Optimizer, Loss Function and Scheduler",
   "id": "5a516e0dc4d7a736"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-06T17:00:07.622252Z",
     "start_time": "2026-02-06T17:00:07.617377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "from transformers import get_linear_schedule_with_warmup"
   ],
   "id": "50558cc654feb83e",
   "outputs": [],
   "execution_count": 56
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
